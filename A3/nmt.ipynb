{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of nmt.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4BIpGwANoQOg",
        "pbvpn4MaV0I1",
        "bRWfRdmVVjUl",
        "0yh08KhgnA30",
        "ecEq4TP2lZ4Z",
        "RWwA6OGqlaTq",
        "AJSafHSAmu_w",
        "73_p8d5EmvOJ",
        "vYPae08Io1Fi",
        "9tcpUFKqo2Oi",
        "z1hDi020rT36",
        "MBnBXRG8mvcn"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjPTaRB4mpCd"
      },
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignmentby selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9IS9B9-yUU5"
      },
      "source": [
        "## Setup PyTorch\n",
        "All files are stored at /content/csc421/a3/ folder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axbuunY8UdTB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-6MQhMOlHXD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "outputId": "a05a212c-2920-4ac5-ae2c-ce8676d2ac6b"
      },
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install torch torchvision\n",
        "!pip install Pillow==4.0.0\n",
        "%mkdir -p ./content/csc421/a3/\n",
        "%cd ./content/csc421/a3"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Collecting pillow>=4.1.1\n",
            "  Using cached https://files.pythonhosted.org/packages/1f/6d/b719ae8e21660a6a962636896dc4b7d657ef451a3ab941516401846ac5cb/Pillow-8.1.2-cp37-cp37m-manylinux1_x86_64.whl\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pillow\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-8.1.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Processing /root/.cache/pip/wheels/4f/0a/2a/7e3391063af230fac4b5fdb4cc93adcb1d99af325b623cea03/Pillow-4.0.0-cp37-cp37m-linux_x86_64.whl\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.7/dist-packages (from Pillow==4.0.0) (0.46)\n",
            "\u001b[31mERROR: torchvision 0.9.0+cu101 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: scikit-image 0.16.2 has requirement pillow>=4.3.0, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 8.1.2\n",
            "    Uninstalling Pillow-8.1.2:\n",
            "      Successfully uninstalled Pillow-8.1.2\n",
            "Successfully installed Pillow-4.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content/content/csc421/a3/content/csc421/a3/content/csc421/a3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DaTdRNuUra7"
      },
      "source": [
        "# Helper code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIpGwANoQOg"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-UJHBYZkh7f"
      },
      "source": [
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "\n",
        "def get_file(fname,\n",
        "             origin,\n",
        "             untar=False,\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir='data'):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + '.tar.gz'\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "    \n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print('Extracting file.')\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "        \n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel('Epochs', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_loss_comparison_lstm(l1, l2, o1, o2, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and val loss curves from LSTM runs.\n",
        "    \n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i*s:(i+1)*s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i*s:(i+1)*s]) for i in range(len(l2[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    ax[0].plot(range(len(mean_l1)), mean_l1, label='ds=' + o1.data_file_name)\n",
        "    ax[0].plot(range(len(mean_l2)), mean_l2, label='ds=' + o2.data_file_name)\n",
        "    ax[0].title.set_text('Train Loss | LSTM Hidden Size = {}'.format(o2.hidden_size))\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[1].plot(range(len(l1[1])), l1[1], label='ds=' + o1.data_file_name)\n",
        "    ax[1].plot(range(len(l2[1])), l2[1], label='ds=' + o2.data_file_name)\n",
        "    ax[1].title.set_text('Val Loss | LSTM Hidden Size = {}'.format(o2.hidden_size))\n",
        "\n",
        "    ax[0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "    ax[0].set_ylabel(\"Loss\", fontsize=10)\n",
        "    ax[1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "    ax[1].set_ylabel(\"Loss\", fontsize=10)\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle('LSTM Performance by Dataset', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.85)\n",
        "    plt.legend()\n",
        "    plt.savefig('./loss_plot_{}.pdf'.format(fn))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_loss_comparison_by_dataset(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
        "    runs in Part 3, comparing by dataset while holding hidden size constant.\n",
        "\n",
        "    Models within each pair (l1, l2) and (l3, l4) have the same hidden sizes.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        l3: Tuple of lists containing training / val losses for model 3.\n",
        "        l4: Tuple of lists containing training / val losses for model 4.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        o3: Options for model 3.\n",
        "        o4: Options for model 4.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i*s:(i+1)*s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i*s:(i+1)*s]) for i in range(len(l2[0]) // s)]\n",
        "    mean_l3 = [np.mean(l3[0][i*s:(i+1)*s]) for i in range(len(l3[0]) // s)]\n",
        "    mean_l4 = [np.mean(l4[0][i*s:(i+1)*s]) for i in range(len(l4[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label='ds=' + o1.data_file_name)\n",
        "    ax[0][0].plot(range(len(mean_l2)), mean_l2, label='ds=' + o2.data_file_name)\n",
        "    ax[0][0].title.set_text('Train Loss | Model Hidden Size = {}'.format(o1.hidden_size))\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[0][1].plot(range(len(l1[1])), l1[1], label='ds=' + o1.data_file_name)\n",
        "    ax[0][1].plot(range(len(l2[1])), l2[1], label='ds=' + o2.data_file_name)\n",
        "    ax[0][1].title.set_text('Val Loss | Model Hidden Size = {}'.format(o1.hidden_size))\n",
        "\n",
        "    ax[1][0].plot(range(len(mean_l3)), mean_l3, label='ds=' + o3.data_file_name)\n",
        "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label='ds=' + o4.data_file_name)\n",
        "    ax[1][0].title.set_text('Train Loss | Model Hidden Size = {}'.format(o3.hidden_size))\n",
        "\n",
        "    ax[1][1].plot(range(len(l3[1])), l3[1], label='ds=' + o3.data_file_name)\n",
        "    ax[1][1].plot(range(len(l4[1])), l4[1], label='ds=' + o4.data_file_name)\n",
        "    ax[1][1].title.set_text('Val Loss | Model Hidden Size = {}'.format(o4.hidden_size))\n",
        "\n",
        "    for i in range(2):\n",
        "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][0].legend(loc=\"upper right\")\n",
        "        ax[i][1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"Performance by Dataset Size\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.9)\n",
        "    plt.legend()\n",
        "    plt.savefig('./loss_plot_{}.pdf'.format(fn))\n",
        "    plt.close()\n",
        "\n",
        "def save_loss_comparison_by_hidden(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
        "    runs in Part 3, comparing by hidden size while holding dataset constant.\n",
        "\n",
        "    Models within each pair (l1, l3) and (l2, l4) have the same dataset.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        l3: Tuple of lists containing training / val losses for model 3.\n",
        "        l4: Tuple of lists containing training / val losses for model 4.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        o3: Options for model 3.\n",
        "        o4: Options for model 4.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i*s:(i+1)*s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i*s:(i+1)*s]) for i in range(len(l2[0]) // s)]\n",
        "    mean_l3 = [np.mean(l3[0][i*s:(i+1)*s]) for i in range(len(l3[0]) // s)]\n",
        "    mean_l4 = [np.mean(l4[0][i*s:(i+1)*s]) for i in range(len(l4[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label='hid_size=' + str(o1.hidden_size))\n",
        "    ax[0][0].plot(range(len(mean_l3)), mean_l3, label='hid_size=' + str(o3.hidden_size))\n",
        "    ax[0][0].title.set_text('Train Loss | Dataset = ' + o1.data_file_name)\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[0][1].plot(range(len(l1[1])), l1[1], label='hid_size=' + str(o1.hidden_size))\n",
        "    ax[0][1].plot(range(len(l3[1])), l3[1], label='hid_size=' + str(o3.hidden_size))\n",
        "    ax[0][1].title.set_text('Val Loss | Dataset = ' + o1.data_file_name)\n",
        "\n",
        "    ax[1][0].plot(range(len(mean_l2)), mean_l2, label='hid_size=' + str(o2.hidden_size))\n",
        "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label='hid_size=' + str(o4.hidden_size))\n",
        "    ax[1][0].title.set_text('Train Loss | Dataset = ' + o3.data_file_name)\n",
        "\n",
        "    ax[1][1].plot(range(len(l2[1])), l2[1], label='hid_size=' + str(o2.hidden_size))\n",
        "    ax[1][1].plot(range(len(l4[1])), l4[1], label='hid_size=' + str(o4.hidden_size))\n",
        "    ax[1][1].title.set_text('Val Loss | Dataset = ' + o4.data_file_name)\n",
        "\n",
        "    for i in range(2):\n",
        "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][0].legend(loc=\"upper right\")\n",
        "        ax[i][1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"Performance by Hidden State Size\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.9)\n",
        "    plt.legend()\n",
        "    plt.savefig('./loss_plot_{}.pdf'.format(fn))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
        "        pkl.dump(idx_dict, f)\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbvpn4MaV0I1"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVT4TNTOV3Eg"
      },
      "source": [
        "def read_lines(filename):\n",
        "    \"\"\"Read a file and split it into lines.\n",
        "    \"\"\"\n",
        "    lines = open(filename).read().strip().lower().split('\\n')\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n",
        "    \"\"\"\n",
        "    return all(c.isalpha() or c == '-' for c in s)\n",
        "\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n",
        "    \"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data(file_name):\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n",
        "    \"\"\"\n",
        "    path = \"./data/{}.txt\".format(file_name)\n",
        "    source_lines, target_lines = read_pairs(path)\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n",
        "\n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n",
        "\n",
        "    # Add start and end tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    char_to_index['SOS'] = start_token\n",
        "    char_to_index['EOS'] = end_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = { index: char for (char, index) in char_to_index.items() }\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "\n",
        "    idx_dict = { 'char_to_index': char_to_index,\n",
        "                 'index_to_char': index_to_char,\n",
        "                 'start_token': start_token,\n",
        "                 'end_token': end_token }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s,t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s,t))\n",
        "\n",
        "    return d\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRWfRdmVVjUl"
      },
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa5-onJhoSeM"
      },
      "source": [
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\n",
        "    \"\"\"\n",
        "    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data(opts['data_file_name'])\n",
        "    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\n",
        "    \"\"\"\n",
        "\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden, encoder_last_cell = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_cell = encoder_last_cell\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden, decoder_cell)\n",
        "\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1] #latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [index_to_char[int(item)] \n",
        "                for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data(opts['data_file_name'])\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden, encoder_cell = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_cell = encoder_cell\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden, decoder_cell)\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1] #latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [index_to_char[int(item)] \n",
        "                for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "    \n",
        "    if isinstance(attention_weights, tuple):\n",
        "      ## transformer's attention mweights\n",
        "      attention_weights, self_attention_weights = attention_weights\n",
        "    \n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "    \n",
        "    for i in range(len(all_attention_weights)):\n",
        "        attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)\n",
        "        cax = ax.matshow(attention_weights_matrix, cmap='bone')\n",
        "        fig.colorbar(cax)\n",
        "\n",
        "        # Set up axes\n",
        "        ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n",
        "        ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n",
        "\n",
        "        # Show label at every tick\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        # Add title\n",
        "        plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n",
        "        plt.tight_layout()\n",
        "        plt.grid('off')\n",
        "        plt.show()\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n",
        "        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden, encoder_cell = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "            decoder_cell = encoder_cell\n",
        "\n",
        "            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n",
        "\n",
        "            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden, decoder_cell)\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "            \n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "              # Zero gradients\n",
        "              optimizer.zero_grad()\n",
        "              # Compute gradients\n",
        "              loss.backward()\n",
        "              # Update the parameters of the encoder and decoder\n",
        "              optimizer.step()\n",
        "\n",
        "    return losses\n",
        "\n",
        "  \n",
        "\n",
        "def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "        * Returns loss curves for comparison\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "    \n",
        "    Returns:\n",
        "        losses: Lists containing training and validation loss curves.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    mean_train_losses = []\n",
        "    mean_val_losses = []\n",
        "\n",
        "    early_stopping_counter = 0\n",
        "    \n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
        "        \n",
        "        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n",
        "        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n",
        "\n",
        "        mean_train_loss = np.mean(train_loss)\n",
        "        mean_val_loss = np.mean(val_loss)\n",
        "\n",
        "        if mean_val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "            best_val_loss = mean_val_loss\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "        \n",
        "        if early_stopping_counter > opts.early_stopping_patience:\n",
        "            print(\"Validation loss has not improved in {} epochs, stopping early\".format(opts.early_stopping_patience))\n",
        "            print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "            return (train_losses, mean_val_losses)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, mean_train_loss, mean_val_loss, gen_string))\n",
        "\n",
        "        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses += train_loss\n",
        "        val_losses += val_loss\n",
        "\n",
        "        mean_train_losses.append(mean_train_loss)\n",
        "        mean_val_losses.append(mean_val_loss)\n",
        "\n",
        "        save_loss_plot(mean_train_losses, mean_val_losses, opts)\n",
        "\n",
        "    print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "    return (train_losses, mean_val_losses)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Data Stats'.center(80))\n",
        "    print('-' * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print('Num unique word pairs: {}'.format(len(line_pairs)))\n",
        "    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n",
        "    print('Vocab size: {}'.format(vocab_size))\n",
        "    print('=' * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data(opts['data_file_name'])\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    if opts.encoder_type == \"rnn\":\n",
        "        encoder = LSTMEncoder(vocab_size=vocab_size, \n",
        "                              hidden_size=opts.hidden_size, \n",
        "                              opts=opts)\n",
        "    elif opts.encoder_type == \"transformer\":\n",
        "        encoder = TransformerEncoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers,\n",
        "                                     opts=opts)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if opts.decoder_type == 'rnn':\n",
        "        decoder = RNNDecoder(vocab_size=vocab_size, \n",
        "                             hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == 'rnn_attention':\n",
        "        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n",
        "                                      hidden_size=opts.hidden_size, \n",
        "                                      attention_type=opts.attention_type)\n",
        "    elif opts.decoder_type == 'transformer':\n",
        "        decoder = TransformerDecoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    #### setup checkpoint path\n",
        "    model_name = 'h{}-bs{}-{}-{}'.format(opts.hidden_size, \n",
        "                                      opts.batch_size, \n",
        "                                      opts.decoder_type,\n",
        "                                      opts.data_file_name)\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n",
        "\n",
        "    try:\n",
        "        losses = training_loop(train_dict, val_dict, idx_dict, encoder, \n",
        "                               decoder, criterion, optimizer, opts)\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return encoder, decoder, losses\n",
        "      \n",
        "    return encoder, decoder, losses\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Opts'.center(80))\n",
        "    print('-' * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
        "    print('=' * 80)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yh08KhgnA30"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aROU2xZanDKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "141b352c-7267-425c-fd6b-f1aa64ee0bdc"
      },
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(fname='pig_latin_small.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_small.txt', \n",
        "                         untar=False)\n",
        "\n",
        "data_fpath = get_file(fname='pig_latin_large.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_large.txt', \n",
        "                         untar=False)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/pig_latin_small.txt\n",
            "data/pig_latin_large.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDYMr7NclZdw"
      },
      "source": [
        "# Part 1: Long Short-Term Memory Unit (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCae1mOUlZrC"
      },
      "source": [
        "## Step 1: LSTM Cell\n",
        "Please implement the Long Short-Term Memory class defined in the next cell. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOnALRQkkjDO"
      },
      "source": [
        "class MyLSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyLSTMCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # ------------\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        self.Wii = nn.Linear(input_size, hidden_size)\n",
        "        self.Whi = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wif = nn.Linear(input_size, hidden_size)\n",
        "        self.Whf = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wig = nn.Linear(input_size, hidden_size)\n",
        "        self.Whg = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wio = nn.Linear(input_size, hidden_size)\n",
        "        self.Who = nn.Linear(hidden_size, hidden_size)\n",
        "        # ------------\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, h_prev, c_prev):\n",
        "        \"\"\"Forward pass of the LSTM computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "            c_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "            c_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "        # ------------\n",
        "        t1 = self.Wif(x)\n",
        "        t2 = self.Whf(h_prev)\n",
        "\n",
        "        f = self.sigmoid(t1 + t2)\n",
        "\n",
        "        t3 = self.Wii(x)\n",
        "        t4 = self.Whi(h_prev)\n",
        "\n",
        "        i = self.sigmoid(t3 + t4)\n",
        "\n",
        "        t5 = self.Wig(x)\n",
        "        t6 = self.Whg(h_prev)\n",
        "\n",
        "        g = self.tanh(t5 + t6)\n",
        "\n",
        "        t7 = self.Wio(x)\n",
        "        t8 = self.Who(h_prev)\n",
        "\n",
        "        o = self.sigmoid(t7 + t8)\n",
        "\n",
        "        c_new = (c_prev*f) + (i * g)\n",
        "        h_new = o * (self.tanh(c_new))\n",
        "\n",
        "        return h_new, c_new\n",
        "        # ------------\n",
        "        # i = ...\n",
        "        # f = ...\n",
        "        # g = ...\n",
        "        # o = ...\n",
        "        # return h_new"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecEq4TP2lZ4Z"
      },
      "source": [
        "## Step 2: LSTM Encoder\n",
        "Please inspect the following recurrent encoder/decoder implementations. Make sure to run the cells before proceeding. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jDNim2fmVJV"
      },
      "source": [
        "class LSTMEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.lstm = MyLSTMCell(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "        cell = self.init_hidden(batch_size)\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "        annotations = []\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n",
        "            hidden, cell = self.lstm(x, hidden, cell)\n",
        "            annotations.append(hidden)\n",
        "\n",
        "        annotations = torch.stack(annotations, dim=1)\n",
        "        return annotations, hidden, cell\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
        "        of a batch of sequences.\n",
        "\n",
        "        Arguments:\n",
        "            bs: The batch size for the initial hidden state.\n",
        "\n",
        "        Returns:\n",
        "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvwizYM9ma4p"
      },
      "source": [
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = MyLSTMCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init, cell_init):\n",
        "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
        "            annotations: This is not used here. It just maintains consistency with the\n",
        "                    interface used by the AttentionDecoder class.\n",
        "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "            cell_init: The cell states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            None\n",
        "        \"\"\"        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "        c_prev = cell_init\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n",
        "            h_prev, c_prev = self.rnn(x, h_prev, c_prev)  # batch_size x hidden_size\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, None  "
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSDTbsydlaGI"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "Train the following language model comprised of recurrent encoder and decoders. \n",
        "\n",
        "First, we train on the smaller dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmVuXTozTPF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "460a0daf-b337-4bb8-f736-fcf9edddbbb1"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "rnn_args_s = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_small',\n",
        "              'cuda':True,\n",
        "              'nepochs':50,\n",
        "              'checkpoint_dir':\"checkpoints\",\n",
        "              'learning_rate':0.005,\n",
        "              'lr_decay':0.99,\n",
        "              'early_stopping_patience':20,\n",
        "              'batch_size':64,\n",
        "              'hidden_size':32,\n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': '',  # options: additive / scaled_dot\n",
        "}\n",
        "rnn_args_s.update(args_dict)\n",
        "\n",
        "print_opts(rnn_args_s)\n",
        "rnn_encode_s, rnn_decoder_s, rnn_losses_s = train(rnn_args_s)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encode_s, rnn_decoder_s, None, rnn_args_s)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 20                                     \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('brother', 'otherbray')\n",
            "('command', 'ommandcay')\n",
            "('collecting', 'ollectingcay')\n",
            "('follows', 'ollowsfay')\n",
            "('seems', 'eemssay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.346 | Val loss: 2.061 | Gen: insay insay insay-ingay insay insay\n",
            "Epoch:   1 | Train loss: 1.898 | Val loss: 1.890 | Gen: ay-ay ay-ay ingsay-ontay-ay-ay-a ingay ingay-ontay\n",
            "Epoch:   2 | Train loss: 1.711 | Val loss: 1.748 | Gen: eday artay ongay-ationtay ingay ontay-ationday\n",
            "Epoch:   3 | Train loss: 1.592 | Val loss: 1.701 | Gen: etay arway ingsay-angay ingway ungay-ay-ay\n",
            "Epoch:   4 | Train loss: 1.499 | Val loss: 1.642 | Gen: etay arway onday-away iway orday\n",
            "Epoch:   5 | Train loss: 1.415 | Val loss: 1.528 | Gen: etay arway ongingsay iway ordingway\n",
            "Epoch:   6 | Train loss: 1.327 | Val loss: 1.458 | Gen: etay arway ongingingway iway ordingway\n",
            "Epoch:   7 | Train loss: 1.248 | Val loss: 1.421 | Gen: etay arway oningingday iway ordingway\n",
            "Epoch:   8 | Train loss: 1.178 | Val loss: 1.350 | Gen: etay ariway ondingingday isway ordingway\n",
            "Epoch:   9 | Train loss: 1.113 | Val loss: 1.307 | Gen: etay arway ondingingday isway ordingway\n",
            "Epoch:  10 | Train loss: 1.053 | Val loss: 1.272 | Gen: etay arway oingingday-oday isway ordingway\n",
            "Epoch:  11 | Train loss: 0.993 | Val loss: 1.216 | Gen: ehay arway oingingday-iteway isway ordingway\n",
            "Epoch:  12 | Train loss: 0.944 | Val loss: 1.192 | Gen: ehay arway ondingingcay isway ordingway\n",
            "Epoch:  13 | Train loss: 0.910 | Val loss: 1.235 | Gen: ehay arway oindingday-inteway isway ordingway\n",
            "Epoch:  14 | Train loss: 0.899 | Val loss: 1.209 | Gen: eway iray ondingingcay isway ordingway\n",
            "Epoch:  15 | Train loss: 0.848 | Val loss: 1.143 | Gen: ehay ariway ondingingday isway oringway\n",
            "Epoch:  16 | Train loss: 0.797 | Val loss: 1.116 | Gen: ehay arway oncingingcay isway oringray\n",
            "Epoch:  17 | Train loss: 0.762 | Val loss: 1.093 | Gen: ehay ariway ondingingcay isway oringway\n",
            "Epoch:  18 | Train loss: 0.731 | Val loss: 1.068 | Gen: ehay ariway ondingintay-iday isway oringway\n",
            "Epoch:  19 | Train loss: 0.704 | Val loss: 1.029 | Gen: ehay ariway oncingingday isway oringway\n",
            "Epoch:  20 | Train loss: 0.676 | Val loss: 1.029 | Gen: ehay ariway ondingincay-iteway isway oringray\n",
            "Epoch:  21 | Train loss: 0.654 | Val loss: 1.039 | Gen: ehay ariway ondingitingday isway oriblay\n",
            "Epoch:  22 | Train loss: 0.632 | Val loss: 1.008 | Gen: ehway ariway oningingday isway orkingway\n",
            "Epoch:  23 | Train loss: 0.610 | Val loss: 1.006 | Gen: ehtway airway ondingincay-ichay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.598 | Val loss: 1.003 | Gen: ehay airway ondingitincay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.574 | Val loss: 0.992 | Gen: ehay airway onicilingay-away isway origmay\n",
            "Epoch:  26 | Train loss: 0.561 | Val loss: 0.988 | Gen: ehay airway ingionday-intedway isway orkingway\n",
            "Epoch:  27 | Train loss: 0.540 | Val loss: 0.994 | Gen: ehtway airway oningitionday isway orignay\n",
            "Epoch:  28 | Train loss: 0.537 | Val loss: 1.005 | Gen: ehay ariway onicitiongday isway origmay\n",
            "Epoch:  29 | Train loss: 0.522 | Val loss: 0.974 | Gen: ehay airway onicitionday isway orkingway\n",
            "Epoch:  30 | Train loss: 0.490 | Val loss: 0.967 | Gen: ehay ariway onicitionday-ilay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.475 | Val loss: 0.954 | Gen: ehay airway onicondingday isway orkingway\n",
            "Epoch:  32 | Train loss: 0.457 | Val loss: 0.952 | Gen: ehay ariway onicitiondedway isway orkingway\n",
            "Epoch:  33 | Train loss: 0.445 | Val loss: 0.964 | Gen: ehay airway onicitioncay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.441 | Val loss: 0.956 | Gen: ehway ariway onicitiondedway isway oriknay\n",
            "Epoch:  35 | Train loss: 0.433 | Val loss: 0.952 | Gen: ehway ariway oncikingday-intway isway orkingway\n",
            "Epoch:  36 | Train loss: 0.422 | Val loss: 0.972 | Gen: ehay ariway ontikingday-inway isway orkingway\n",
            "Epoch:  37 | Train loss: 0.423 | Val loss: 0.951 | Gen: ehway ariway onicitiondedway isway oriknay\n",
            "Epoch:  38 | Train loss: 0.395 | Val loss: 0.937 | Gen: ehay ariway oniconidinceway isway orkingway\n",
            "Epoch:  39 | Train loss: 0.381 | Val loss: 0.926 | Gen: ehay airway onicitiondedway isway oriknay\n",
            "Epoch:  40 | Train loss: 0.370 | Val loss: 0.955 | Gen: ethay ariway ontinitionday isway orkingway\n",
            "Epoch:  41 | Train loss: 0.366 | Val loss: 0.940 | Gen: ehway ariway onicitionvanway isway oriknay\n",
            "Epoch:  42 | Train loss: 0.359 | Val loss: 0.938 | Gen: etway ariway onicondingday isway orkingway\n",
            "Epoch:  43 | Train loss: 0.352 | Val loss: 0.935 | Gen: ehway airway ondintiondicway isway oriknray\n",
            "Epoch:  44 | Train loss: 0.350 | Val loss: 0.953 | Gen: etway ariway ondicitioncay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.351 | Val loss: 0.968 | Gen: ehway ariway onicondingday isway orkingway\n",
            "Epoch:  46 | Train loss: 0.368 | Val loss: 0.959 | Gen: ethay arilay ondinticedingay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.364 | Val loss: 0.931 | Gen: ehway airway ondingicay-iteway isway orkingway\n",
            "Epoch:  48 | Train loss: 0.343 | Val loss: 0.909 | Gen: ehay ariway oniconingday isway oriknay\n",
            "Epoch:  49 | Train loss: 0.329 | Val loss: 0.912 | Gen: ehay airway ontinitiondvay isway orkingway\n",
            "Obtained lowest validation loss of: 0.9090394865427363\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehay airway ontinitiondvay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mR97V_NtER6"
      },
      "source": [
        "Next, we train on the larger dataset. This experiment investigates if increasing dataset size improves model generalization on the validation set. \n",
        "\n",
        "For a fair comparison, the number of iterations (not number of epochs) for each run should be similar. This is done in a rough and dirty way by adjusting the batch size so approximately the same number of batches is processed per epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3YLrAjsmx_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5285a58b-c0ff-4fa5-99b6-5124c418f131"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "rnn_args_l = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_large',\n",
        "              'cuda':True,\n",
        "              'nepochs':50,\n",
        "              'checkpoint_dir':\"checkpoints\",\n",
        "              'learning_rate':0.005,\n",
        "              'lr_decay':0.99,\n",
        "              'early_stopping_patience':10,\n",
        "              'batch_size':512,\n",
        "              'hidden_size':32,\n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': '',  # options: additive / scaled_dot\n",
        "}\n",
        "rnn_args_l.update(args_dict)\n",
        "\n",
        "print_opts(rnn_args_l)\n",
        "rnn_encode_l, rnn_decoder_l, rnn_losses_l = train(rnn_args_l)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encode_l, rnn_decoder_l, None, rnn_args_l)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 10                                     \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('trustees', 'usteestray')\n",
            "('negro', 'egronay')\n",
            "('avant', 'avantway')\n",
            "('pfizer', 'izerpfay')\n",
            "('excel', 'excelway')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.312 | Val loss: 2.115 | Gen: ay ay ontay-intay-ay-ay ay ontay-ay-ay\n",
            "Epoch:   1 | Train loss: 1.841 | Val loss: 1.913 | Gen: eray ay-ay-ay ontertay-ingay ingay ontay-ay-ay\n",
            "Epoch:   2 | Train loss: 1.652 | Val loss: 1.792 | Gen: eray aray-ay intingay-ingay ingay ontay-ingay\n",
            "Epoch:   3 | Train loss: 1.495 | Val loss: 1.666 | Gen: ertay arway intenstay-ingay isay ontay-ingay\n",
            "Epoch:   4 | Train loss: 1.376 | Val loss: 1.635 | Gen: eway arway ontingergay-ay isway ontay-ingay\n",
            "Epoch:   5 | Train loss: 1.262 | Val loss: 1.488 | Gen: ethay arway ontingnarsay isway orvay-ingay\n",
            "Epoch:   6 | Train loss: 1.155 | Val loss: 1.466 | Gen: eway arway ontingsay-orngay isway orkerway\n",
            "Epoch:   7 | Train loss: 1.076 | Val loss: 1.388 | Gen: ethay arway ontingsay-insay isway orvinway\n",
            "Epoch:   8 | Train loss: 1.011 | Val loss: 1.386 | Gen: ethay ariway ontingstay-anchay isway orkingway\n",
            "Epoch:   9 | Train loss: 0.950 | Val loss: 1.336 | Gen: etway ariway oningtaray-istay isway inkoray\n",
            "Epoch:  10 | Train loss: 0.892 | Val loss: 1.290 | Gen: ethay ariway ongingedtay isway ovinway\n",
            "Epoch:  11 | Train loss: 0.851 | Val loss: 1.435 | Gen: ethway ariway ongintestingway istay orkenway\n",
            "Epoch:  12 | Train loss: 0.837 | Val loss: 1.278 | Gen: ethay aiway ontingraway-ompay isway orkingray\n",
            "Epoch:  13 | Train loss: 0.794 | Val loss: 1.205 | Gen: ethay ariway ontingsay-incay-aysa isway orkingway\n",
            "Epoch:  14 | Train loss: 0.738 | Val loss: 1.196 | Gen: ethay ariway ongingtingway isway inkday-orfay\n",
            "Epoch:  15 | Train loss: 0.706 | Val loss: 1.205 | Gen: ethay airay ontingicationay isway orkingway\n",
            "Epoch:  16 | Train loss: 0.686 | Val loss: 1.154 | Gen: ethay airway oningictay-onday isway inkday-orfay\n",
            "Epoch:  17 | Train loss: 0.660 | Val loss: 1.149 | Gen: ethay airay ontingicingway isway orkingway\n",
            "Epoch:  18 | Train loss: 0.647 | Val loss: 1.166 | Gen: ethay ariway onioingway-icatay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.631 | Val loss: 1.164 | Gen: ethay airway inotingsay-awlay isway inkway-ashay\n",
            "Epoch:  20 | Train loss: 0.608 | Val loss: 1.137 | Gen: ethay ariway onizionday-atisay isway okiraway\n",
            "Epoch:  21 | Train loss: 0.582 | Val loss: 1.080 | Gen: ethay airway ontingsay-incepay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.548 | Val loss: 1.089 | Gen: ethay ariway ontiongingway isway orkingway\n",
            "Epoch:  23 | Train loss: 0.538 | Val loss: 1.097 | Gen: ethay ariway oningioditay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.531 | Val loss: 1.036 | Gen: ethay airway ontingicingsway isway ikndraypay\n",
            "Epoch:  25 | Train loss: 0.507 | Val loss: 1.051 | Gen: ethay ariway ontingionsday isway orkingway\n",
            "Epoch:  26 | Train loss: 0.495 | Val loss: 1.080 | Gen: ethay airway ontingickay-astay isway owkingray\n",
            "Epoch:  27 | Train loss: 0.493 | Val loss: 1.057 | Gen: ethay ariway ontigingshay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.480 | Val loss: 1.107 | Gen: ethay ariway onioingtay-ashlay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.469 | Val loss: 1.109 | Gen: ethay ariway ontingiondicay isway iwkindgray\n",
            "Epoch:  30 | Train loss: 0.483 | Val loss: 1.045 | Gen: ethay airway ontizionsgay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.448 | Val loss: 1.096 | Gen: ethay airway ontingionsdway isway orkingway\n",
            "Epoch:  32 | Train loss: 0.438 | Val loss: 1.042 | Gen: ethay airway ontizionsay-ipway isway orkingway\n",
            "Epoch:  33 | Train loss: 0.423 | Val loss: 1.041 | Gen: ethay airway ontizionday-ashay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.419 | Val loss: 1.042 | Gen: ethay airway ontizionsay-actray isway orkingway\n",
            "Epoch:  35 | Train loss: 0.402 | Val loss: 1.022 | Gen: ethay airway ontioingingway isway orkingway\n",
            "Epoch:  36 | Train loss: 0.394 | Val loss: 1.011 | Gen: ethay airway ontizionday-acpway isway orkingway\n",
            "Epoch:  37 | Train loss: 0.377 | Val loss: 1.001 | Gen: ethay airway ontionigtincay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.367 | Val loss: 1.000 | Gen: ethay airway ontiziondascyway isway orkingway\n",
            "Epoch:  39 | Train loss: 0.355 | Val loss: 0.985 | Gen: ethay airway ontionigingsway isway orkingway\n",
            "Epoch:  40 | Train loss: 0.350 | Val loss: 1.011 | Gen: ethay airway ontingiodincway isway orkingway\n",
            "Epoch:  41 | Train loss: 0.349 | Val loss: 0.969 | Gen: ethay airway ontioingidgway isway orkingway\n",
            "Epoch:  42 | Train loss: 0.344 | Val loss: 1.009 | Gen: ethay airway ontiodinginway isway orkingway\n",
            "Epoch:  43 | Train loss: 0.352 | Val loss: 1.011 | Gen: ethay airway ontingiodincay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.354 | Val loss: 1.094 | Gen: ethay ariway ontiongidghay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.347 | Val loss: 1.039 | Gen: ethay airway ontioingdicay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.346 | Val loss: 0.988 | Gen: ethay airway ontiodingnay isway orkinaway\n",
            "Epoch:  47 | Train loss: 0.325 | Val loss: 0.971 | Gen: ethay airway ontioingingsway isway orkingway\n",
            "Epoch:  48 | Train loss: 0.309 | Val loss: 0.988 | Gen: ethay airway ontionidgalway isway orkingway\n",
            "Epoch:  49 | Train loss: 0.297 | Val loss: 0.988 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Obtained lowest validation loss of: 0.9687766670888546\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01HsZ6EItc56"
      },
      "source": [
        "The code below plots the training and validation losses of each model, as a function of the number of gradient descent iterations. Consider if there are significant differences in the validation performance of each model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Qyk_9-Fwtekj",
        "outputId": "9d3f02ce-bef3-4915-e78f-14a646fcdf30"
      },
      "source": [
        "save_loss_comparison_lstm(rnn_losses_s, rnn_losses_l, rnn_args_s, rnn_args_l, 'lstm')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE4ijaCzneAt"
      },
      "source": [
        "Select best performing model, and try translating different sentences by changing the variable TEST_SENTENCE. Identify a failure mode and briefly describe it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrNnz8W1nULf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5370e208-3916-4504-865b-832c4d66fb11"
      },
      "source": [
        "best_encoder = rnn_encode_s # Replace with rnn_losses_s or rnn_losses l\n",
        "best_decoder = rnn_decoder_s # etc.\n",
        "best_args = rnn_args_s\n",
        "\n",
        "TEST_SENTENCE = 'greedy seedy and needy street'\n",
        "translated = translate_sentence(TEST_SENTENCE, best_encoder, best_decoder, None, best_args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tgreedy seedy and needy street \n",
            "translated:\teedigbay eedeway andway eedingway eetechay\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWwA6OGqlaTq"
      },
      "source": [
        "# Part 2: Additive Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJSafHSAmu_w"
      },
      "source": [
        "## Step 1: Additive Attention\n",
        "Already implemented the additive attention mechanism. Write down the mathematical expression for $\\tilde{\\alpha}_i^{(t)}, \\alpha_i^{(t)}, c_t$ as a function of $W_1, W_2, b_1, b_2, Q_t, K_i$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdewEVSMo5jJ"
      },
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # A two layer fully-connected network\n",
        "        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n",
        "        self.attention_network = nn.Sequential(\n",
        "                                    nn.Linear(hidden_size*2, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(hidden_size, 1)\n",
        "                                 )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the additive attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "        batch_size = keys.size(0)\n",
        "        expanded_queries = queries.view(batch_size, -1, self.hidden_size).expand_as(keys)\n",
        "        concat_inputs = torch.cat([expanded_queries, keys], dim=2)\n",
        "        unnormalized_attention = self.attention_network(concat_inputs)\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(attention_weights.transpose(2,1), values)\n",
        "        return context, attention_weights\n",
        "      "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73_p8d5EmvOJ"
      },
      "source": [
        "## Step 2: RNN Additive Attention Decoder\n",
        "We will now implement a recurrent decoder that makes use of the additive attention mechanism. Read the description in the assignment worksheet and complete the following implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJaABkXrpJSw"
      },
      "source": [
        "class RNNAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n",
        "        super(RNNAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.rnn = MyLSTMCell(input_size=hidden_size*2, hidden_size=hidden_size)\n",
        "        if attention_type == 'additive':\n",
        "          self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
        "        elif attention_type == 'scaled_dot':\n",
        "          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init, cell_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "            cell_init: The final cell states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        attentions = []\n",
        "        h_prev = hidden_init\n",
        "        c_prev = cell_init\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            embed_current = embed[:,i,:]  # Get the current time step, across the whole batch\n",
        "            context, attention_weights = self.attention(h_prev, annotations, annotations)  # batch_size x 1 x hidden_size\n",
        "            embed_and_context = torch.cat([embed_current, context.squeeze(1)], dim=1)  # batch_size x (2*hidden_size)\n",
        "            h_prev, c_prev = self.rnn(embed_and_context, h_prev, c_prev)  # batch_size x hidden_size            \n",
        "            \n",
        "            \n",
        "            hiddens.append(h_prev)\n",
        "            attentions.append(attention_weights)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        attentions = torch.cat(attentions, dim=2) # batch_size x seq_len x seq_len\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, attentions\n",
        "        "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYPae08Io1Fi"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "Train the following language model that uses a recurrent encoder, and a recurrent decoder that has an additive attention component. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke6t6rCezpZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "973fc9fc-5867-4760-970d-fcf8031818c9"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "rnn_attn_args = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_small',\n",
        "              'cuda':True, \n",
        "              'nepochs':50, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005,\n",
        "              'lr_decay':0.99,\n",
        "              'early_stopping_patience':10,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':64, \n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': 'additive',  # options: additive / scaled_dot\n",
        "}\n",
        "rnn_attn_args.update(args_dict)\n",
        "\n",
        "print_opts(rnn_attn_args)\n",
        "rnn_attn_encoder, rnn_attn_decoder, rnn_attn_losses = train(rnn_attn_args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 10                                     \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn_attention                          \n",
            "                         attention_type: additive                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('brother', 'otherbray')\n",
            "('command', 'ommandcay')\n",
            "('collecting', 'ollectingcay')\n",
            "('follows', 'ollowsfay')\n",
            "('seems', 'eemssay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.097 | Val loss: 1.886 | Gen: ingay insay inontay-ontay illay ontay-ontay\n",
            "Epoch:   1 | Train loss: 1.585 | Val loss: 1.564 | Gen: etay arway oncingay-onday-onday isway onsay-ourway\n",
            "Epoch:   2 | Train loss: 1.267 | Val loss: 1.356 | Gen: etay arway ountingingway issay onssay\n",
            "Epoch:   3 | Train loss: 1.024 | Val loss: 1.216 | Gen: ehay arway oudingnay-oursay isway orkingsay\n",
            "Epoch:   4 | Train loss: 0.877 | Val loss: 1.120 | Gen: etay arway ondingcay-ountingway isway ingshay\n",
            "Epoch:   5 | Train loss: 0.744 | Val loss: 0.982 | Gen: ehay arway ondingingcay-otingwa isway onibray\n",
            "Epoch:   6 | Train loss: 0.604 | Val loss: 0.852 | Gen: ethay arway ondingicingway isway orkingway\n",
            "Epoch:   7 | Train loss: 0.510 | Val loss: 0.807 | Gen: ethay airway onditioninglay isway orkingway\n",
            "Epoch:   8 | Train loss: 0.417 | Val loss: 0.689 | Gen: ehay airway ondionitingcay isway orkingway\n",
            "Epoch:   9 | Train loss: 0.341 | Val loss: 0.695 | Gen: ethay airway ondicingingay isway orkingway\n",
            "Epoch:  10 | Train loss: 0.281 | Val loss: 0.637 | Gen: ethay airway ondicingay-ounway isway orkingway\n",
            "Epoch:  11 | Train loss: 0.231 | Val loss: 0.511 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  12 | Train loss: 0.179 | Val loss: 0.467 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  13 | Train loss: 0.136 | Val loss: 0.406 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  14 | Train loss: 0.107 | Val loss: 0.369 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  15 | Train loss: 0.079 | Val loss: 0.336 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  16 | Train loss: 0.068 | Val loss: 0.350 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  17 | Train loss: 0.072 | Val loss: 0.378 | Gen: ethay airway onditioningcay isway orkingjay\n",
            "Epoch:  18 | Train loss: 0.078 | Val loss: 0.374 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.075 | Val loss: 0.338 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.070 | Val loss: 0.419 | Gen: ethay airway onditionitingway isway orkingway\n",
            "Epoch:  21 | Train loss: 0.095 | Val loss: 0.409 | Gen: ethay airway onditioningay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.073 | Val loss: 0.377 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.055 | Val loss: 0.282 | Gen: ethay airway onditingingcay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.035 | Val loss: 0.255 | Gen: ethay airway onditingingcay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.021 | Val loss: 0.247 | Gen: ethay airway onditionicingcay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.015 | Val loss: 0.245 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.012 | Val loss: 0.237 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.010 | Val loss: 0.234 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.008 | Val loss: 0.236 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.007 | Val loss: 0.235 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.007 | Val loss: 0.233 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.006 | Val loss: 0.234 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.005 | Val loss: 0.234 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.005 | Val loss: 0.233 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.004 | Val loss: 0.234 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.004 | Val loss: 0.234 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.004 | Val loss: 0.234 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.004 | Val loss: 0.235 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.003 | Val loss: 0.234 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.003 | Val loss: 0.235 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.003 | Val loss: 0.235 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Validation loss has not improved in 10 epochs, stopping early\n",
            "Obtained lowest validation loss of: 0.23334109729866645\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNVKbLc0ACj_"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw_GOIvzo1ix"
      },
      "source": [
        "# Part 3: Scaled Dot Product Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq7nhsEio1w-"
      },
      "source": [
        "## Step 1: Implement Dot-Product Attention\n",
        "Implement the scaled dot product attention module described in the assignment worksheet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_j3oY3hqsJQ"
      },
      "source": [
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        q = self.Q(queries)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "        \n",
        "        unnormalized_attention = (k @ torch.transpose(q, 1 , 2)) * self.scaling_factor\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.transpose(attention_weights, 1, 2) @ v \n",
        "\n",
        "        return context, attention_weights\n",
        "        # ------------\n",
        "        # batch_size = ...\n",
        "        # q = ...\n",
        "        # k = ...\n",
        "        # v = ...\n",
        "        # unnormalized_attention = ...\n",
        "        # attention_weights = ...\n",
        "        # context = ...\n",
        "        # return context, attention_weights"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unReAOrjo113"
      },
      "source": [
        "## Step 2: Implement Causal Dot-Product Attention\n",
        "Now implement the scaled causal dot product described in the assignment worksheet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovigzQffrKqj"
      },
      "source": [
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e7)\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------    \n",
        "        q = self.Q(queries)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "   \n",
        "        unnormalized_attention = (k @ torch.transpose(q, 1, 2)) * self.scaling_factor\n",
        "        mask = torch.triu(unnormalized_attention)\n",
        "        neginf_1 = self.neg_inf.to(mask.device)\n",
        "        mask[mask==0] = neginf_1\n",
        "       \n",
        "        attention_weights = self.softmax(mask)\n",
        "   \n",
        "        context = torch.transpose(attention_weights, 1, 2) @ v \n",
        "\n",
        "        return context, attention_weights\n",
        "        # ------------\n",
        "        # batch_size = ...\n",
        "        # q = ...\n",
        "        # k = ...\n",
        "        # v = ...\n",
        "        # unnormalized_attention = ...\n",
        "        # mask = ...\n",
        "        # attention_weights = ...\n",
        "        # context = ...\n",
        "        # return context, attention_weights"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tcpUFKqo2Oi"
      },
      "source": [
        "## Step 3: Transformer Encoder\n",
        "Complete the following transformer encoder implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3B-fWsarlVk"
      },
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers, opts):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            None: Used to conform to standard encoder return signature.\n",
        "            None: Used to conform to standard encoder return signature.        \n",
        "        \"\"\"\n",
        "        batch_size, seq_len = inputs.size()\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        # Add positinal embeddings from self.create_positional_encodings. (a'la https://arxiv.org/pdf/1706.03762.pdf, section 3.5)\n",
        "        encoded = encoded + self.positional_encodings[:seq_len]\n",
        "\n",
        "        annotations = encoded\n",
        "        for i in range(self.num_layers):\n",
        "            new_annotations, self_attention_weights = self.self_attentions[i](annotations, annotations, annotations)  # batch_size x seq_len x hidden_size\n",
        "            residual_annotations = annotations + new_annotations\n",
        "            new_annotations = self.attention_mlps[i](residual_annotations)\n",
        "            annotations = residual_annotations + new_annotations\n",
        "\n",
        "        # Transformer encoder does not have a last hidden or cell layer. \n",
        "        return annotations, None, None\n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "        dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n",
        "        exponents = (2*dim_indices).float()/(self.hidden_size)\n",
        "        trig_args = pos_indices / (10000**exponents)\n",
        "        sin_terms = torch.sin(trig_args)\n",
        "        cos_terms = torch.cos(trig_args)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms\n",
        "        pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "        if self.opts.cuda:\n",
        "            pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1hDi020rT36"
      },
      "source": [
        "## Step 4: Transformer Decoder\n",
        "Complete the following transformer decoder implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyvTZFxtrvc6"
      },
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init, cell_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "            cell_init: Not used in transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        embed = embed + self.positional_encodings[:seq_len]\n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        for i in range(self.num_layers):\n",
        "            new_contexts, self_attention_weights = self.self_attentions[i](contexts, contexts, contexts)  # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = contexts + new_contexts\n",
        "            new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts, annotations, annotations) # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = residual_contexts + new_contexts\n",
        "            new_contexts = self.attention_mlps[i](residual_contexts)\n",
        "            contexts = residual_contexts + new_contexts\n",
        "\n",
        "            encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "            self_attention_weights_list.append(self_attention_weights)\n",
        "          \n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "        \n",
        "        return output, (encoder_attention_weights, self_attention_weights)\n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "        dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n",
        "        exponents = (2*dim_indices).float()/(self.hidden_size)\n",
        "        trig_args = pos_indices / (10000**exponents)\n",
        "        sin_terms = torch.sin(trig_args)\n",
        "        cos_terms = torch.cos(trig_args)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms\n",
        "        pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "        pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ZjkXTNrUKb"
      },
      "source": [
        "\n",
        "## Step 5: Training and analysis\n",
        "Now, train the following language model that's comprised of a (simplified) transformer encoder and transformer decoder. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqTp-eCPuuFO"
      },
      "source": [
        "First, we train our smaller model on the small dataset. Use this model to answer Question 4 in the handout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk8e4KSnuZ8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0348a88a-eba5-448d-aedb-c59b473598fb"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "trans32_args_s = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_small',\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':5e-4,\n",
        "              'early_stopping_patience': 100,\n",
        "              'lr_decay':0.99,\n",
        "              'batch_size': 64,\n",
        "              'hidden_size': 32,\n",
        "              'encoder_type': 'transformer',\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "trans32_args_s.update(args_dict)\n",
        "print_opts(trans32_args_s)\n",
        "\n",
        "trans32_encoder_s, trans32_decoder_s, trans32_losses_s = train(trans32_args_s)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 100                                    \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('brother', 'otherbray')\n",
            "('command', 'ommandcay')\n",
            "('collecting', 'ollectingcay')\n",
            "('follows', 'ollowsfay')\n",
            "('seems', 'eemssay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.732 | Val loss: 2.332 | Gen: ay iay inainainay iay inay\n",
            "Epoch:   1 | Train loss: 2.081 | Val loss: 2.114 | Gen: araaeaaaay iay inininay-onay-onay-o isay inay-onay\n",
            "Epoch:   2 | Train loss: 1.859 | Val loss: 1.949 | Gen: eay aray inininay-onay-onay-o iay inay-onay\n",
            "Epoch:   3 | Train loss: 1.702 | Val loss: 1.816 | Gen: elay arararay inininay-onay-iay-io isway oroooooray-oray\n",
            "Epoch:   4 | Train loss: 1.596 | Val loss: 1.728 | Gen: elay-ay arway iningingway-inay-ing isway orongray-ongray\n",
            "Epoch:   5 | Train loss: 1.475 | Val loss: 1.670 | Gen: elway-ay arway-ray inininginay-inay-ioo isway ooringray-ongray\n",
            "Epoch:   6 | Train loss: 1.378 | Val loss: 1.599 | Gen: elway-ay arway oonioonanioninay-ioo issway oorongray-ongray\n",
            "Epoch:   7 | Train loss: 1.299 | Val loss: 1.596 | Gen: elway-ay arrway ooniningatincay-inio issway oorongray\n",
            "Epoch:   8 | Train loss: 1.250 | Val loss: 1.562 | Gen: elway-ay arrway oonioongatincay-ionc isway oongrooongray\n",
            "Epoch:   9 | Train loss: 1.221 | Val loss: 1.486 | Gen: ethay araray oningatingncay isway ongray-ongray\n",
            "Epoch:  10 | Train loss: 1.156 | Val loss: 1.406 | Gen: ethay arway ooongooooongnay-ioio isway ooongray-ongray\n",
            "Epoch:  11 | Train loss: 1.084 | Val loss: 1.357 | Gen: ethay arway ooninganincay-ay isway oongray-ongay\n",
            "Epoch:  12 | Train loss: 1.025 | Val loss: 1.340 | Gen: ethay arway oningatinincay isway oongoray-ongay\n",
            "Epoch:  13 | Train loss: 0.977 | Val loss: 1.321 | Gen: ethay arway oniningatincay isway oongoray\n",
            "Epoch:  14 | Train loss: 0.934 | Val loss: 1.294 | Gen: ethay arway oniningatincay isway oongoray\n",
            "Epoch:  15 | Train loss: 0.896 | Val loss: 1.303 | Gen: eteay arway oninioicay isway oongoray-oway\n",
            "Epoch:  16 | Train loss: 0.881 | Val loss: 1.310 | Gen: emay ariray onionanicay isway ovongway-oway\n",
            "Epoch:  17 | Train loss: 0.842 | Val loss: 1.271 | Gen: ethay ariraway oninicaioiongway isway ovongway-oway\n",
            "Epoch:  18 | Train loss: 0.810 | Val loss: 1.258 | Gen: eathay arway oninicaioiongfay isway ovongway-oway\n",
            "Epoch:  19 | Train loss: 0.772 | Val loss: 1.192 | Gen: ethay ariray ondiconationcay isway ovwongway\n",
            "Epoch:  20 | Train loss: 0.745 | Val loss: 1.162 | Gen: eway ariray ondinicatincay isway ovwongway\n",
            "Epoch:  21 | Train loss: 0.712 | Val loss: 1.157 | Gen: emeway arirway ondincingfay isway ovwongway\n",
            "Epoch:  22 | Train loss: 0.689 | Val loss: 1.218 | Gen: emay arirway ondincincay isway ovwongwrway\n",
            "Epoch:  23 | Train loss: 0.677 | Val loss: 1.320 | Gen: emeway arrrway ondnincanincay isway ovwkngway-way\n",
            "Epoch:  24 | Train loss: 0.668 | Val loss: 1.252 | Gen: emay arirway ondincincay-onc isway ovwkwoway-wngway\n",
            "Epoch:  25 | Train loss: 0.647 | Val loss: 1.236 | Gen: emeway arirway ondinconcay isway ovwkwoway-wrway\n",
            "Epoch:  26 | Train loss: 0.620 | Val loss: 1.130 | Gen: ehay-eaaaay arirway ondinincay-onc isway owkwkwoway\n",
            "Epoch:  27 | Train loss: 0.587 | Val loss: 1.118 | Gen: emeway arirway ondinicainc-ongway isway owkwkwoway\n",
            "Epoch:  28 | Train loss: 0.561 | Val loss: 1.135 | Gen: eheway arirway ondininctingncay isway owkwkwoway\n",
            "Epoch:  29 | Train loss: 0.544 | Val loss: 1.169 | Gen: emeway arirway ondininctingncay isway owkwkwrway\n",
            "Epoch:  30 | Train loss: 0.533 | Val loss: 1.147 | Gen: emeway arirway ondininctay isway owkwkwrway\n",
            "Epoch:  31 | Train loss: 0.523 | Val loss: 1.169 | Gen: emeway arirway ondinciningfincay isway owkwkwoway\n",
            "Epoch:  32 | Train loss: 0.574 | Val loss: 1.060 | Gen: ehay ariray ondicinicay isway ovwvwkway\n",
            "Epoch:  33 | Train loss: 0.542 | Val loss: 1.117 | Gen: ethay ariray ondniconcay isway ovwkwwrway\n",
            "Epoch:  34 | Train loss: 0.540 | Val loss: 1.105 | Gen: emayway arway ondicingfay isway owkwwrway\n",
            "Epoch:  35 | Train loss: 0.554 | Val loss: 1.023 | Gen: efay airray ondnictiongfinay isway owkwkwrway\n",
            "Epoch:  36 | Train loss: 0.515 | Val loss: 1.051 | Gen: ethay arrway ondanictincay isway owkwrway\n",
            "Epoch:  37 | Train loss: 0.516 | Val loss: 0.991 | Gen: ethay arirway ondiciningfay isway owkwrway\n",
            "Epoch:  38 | Train loss: 0.461 | Val loss: 0.986 | Gen: ethay arirway ondiciningfay isway owkwkwrway\n",
            "Epoch:  39 | Train loss: 0.430 | Val loss: 0.967 | Gen: ethay arirway ondiciningfay isway owkwkwrway\n",
            "Epoch:  40 | Train loss: 0.412 | Val loss: 0.942 | Gen: ethay arirway ondiciningfay isway owkwkwrway\n",
            "Epoch:  41 | Train loss: 0.397 | Val loss: 0.927 | Gen: ethay arirway ondiciningfay isway owkwkwrway\n",
            "Epoch:  42 | Train loss: 0.386 | Val loss: 0.926 | Gen: ethay arirway ondiciningfay isway owkwkwrway\n",
            "Epoch:  43 | Train loss: 0.374 | Val loss: 0.921 | Gen: ethay arirway ondiciningfay isway owkwkwrway\n",
            "Epoch:  44 | Train loss: 0.364 | Val loss: 0.920 | Gen: ethay arirway ondiciningfay isway owkwkwrway\n",
            "Epoch:  45 | Train loss: 0.353 | Val loss: 0.919 | Gen: ethay arirway ondiciningfay isway owkwkwrway\n",
            "Epoch:  46 | Train loss: 0.344 | Val loss: 0.919 | Gen: ethay arirway ondiciningfay isway owkwkwrway\n",
            "Epoch:  47 | Train loss: 0.335 | Val loss: 0.921 | Gen: ethay arirway ondiciningfay isway owkwkwrway\n",
            "Epoch:  48 | Train loss: 0.327 | Val loss: 0.921 | Gen: ethay arirway ondiciningfay isway owkwkwrway\n",
            "Epoch:  49 | Train loss: 0.319 | Val loss: 0.922 | Gen: ethay arirway ondiciningfay isway owkwkwrway\n",
            "Epoch:  50 | Train loss: 0.311 | Val loss: 0.921 | Gen: ethay arirway ondiciingnay isway owkwkwrway\n",
            "Epoch:  51 | Train loss: 0.313 | Val loss: 0.964 | Gen: ethay arirway ondwaionitingfingcay isway owkwkwway\n",
            "Epoch:  52 | Train loss: 0.345 | Val loss: 0.952 | Gen: ethay arirway ondiciingfay isway owkwkwrway\n",
            "Epoch:  53 | Train loss: 0.316 | Val loss: 0.908 | Gen: ethay arirway ondicioningfinay isway owkwkwrway\n",
            "Epoch:  54 | Train loss: 0.302 | Val loss: 0.931 | Gen: ethay arirway ondiciingfay isway owkwkwrway\n",
            "Epoch:  55 | Train loss: 0.297 | Val loss: 0.936 | Gen: ethay arirway ondnictiongcay isway owkwkwrway\n",
            "Epoch:  56 | Train loss: 0.307 | Val loss: 0.916 | Gen: ethay arirway onditiincay isway owkwkwway\n",
            "Epoch:  57 | Train loss: 0.281 | Val loss: 0.888 | Gen: ethay arirway onditincay isway owkwkwway\n",
            "Epoch:  58 | Train loss: 0.263 | Val loss: 0.884 | Gen: ethay arirway onditinicay isway owkwkwway\n",
            "Epoch:  59 | Train loss: 0.256 | Val loss: 0.889 | Gen: ethay arirway onditinicay isway owkwkway\n",
            "Epoch:  60 | Train loss: 0.250 | Val loss: 0.888 | Gen: ethay arirway onditincay isway owkwkwway\n",
            "Epoch:  61 | Train loss: 0.245 | Val loss: 0.893 | Gen: ethay arirway onditincay isway owkwkwway\n",
            "Epoch:  62 | Train loss: 0.239 | Val loss: 0.893 | Gen: ethway arirway onditincay isway owkwkwway\n",
            "Epoch:  63 | Train loss: 0.234 | Val loss: 0.903 | Gen: ethway arirway ondnictioncay isway owkwkwway\n",
            "Epoch:  64 | Train loss: 0.229 | Val loss: 0.907 | Gen: ethway arirway onditincay isway owkwkwway\n",
            "Epoch:  65 | Train loss: 0.225 | Val loss: 0.910 | Gen: ethway arirway onditinctay isway owkwkwway\n",
            "Epoch:  66 | Train loss: 0.221 | Val loss: 0.918 | Gen: ethay arirway ondnitioncay isway owkwkwway\n",
            "Epoch:  67 | Train loss: 0.217 | Val loss: 0.914 | Gen: ethway arirway onditincay isway owkwkwway\n",
            "Epoch:  68 | Train loss: 0.215 | Val loss: 0.930 | Gen: ethay arirway ondnitioncay isway owkwkway\n",
            "Epoch:  69 | Train loss: 0.209 | Val loss: 0.929 | Gen: ethway arirway onditiincay isway owkwkwway\n",
            "Epoch:  70 | Train loss: 0.208 | Val loss: 0.936 | Gen: ethway arirway onditiningcay isway owkwkway\n",
            "Epoch:  71 | Train loss: 0.204 | Val loss: 0.934 | Gen: ethway arirway ondnitioncway isway owkwkwway\n",
            "Epoch:  72 | Train loss: 0.205 | Val loss: 0.946 | Gen: ethay arirway onditiincay isway owkwkwway\n",
            "Epoch:  73 | Train loss: 0.245 | Val loss: 1.102 | Gen: ethay arway onditincay isway owkwkway\n",
            "Epoch:  74 | Train loss: 0.296 | Val loss: 1.132 | Gen: ethway arway ondnitionctingway isway owkwkwwwwwwwwwwwwwww\n",
            "Epoch:  75 | Train loss: 0.336 | Val loss: 1.005 | Gen: ethay arway onditiongway isway owkwkwway\n",
            "Epoch:  76 | Train loss: 0.270 | Val loss: 1.037 | Gen: ethmay arirway ondnitioncationcway isway owkwkwrway\n",
            "Epoch:  77 | Train loss: 0.236 | Val loss: 0.876 | Gen: ethay arirway onditionctincay isway owkwkway\n",
            "Epoch:  78 | Train loss: 0.207 | Val loss: 0.898 | Gen: ethay arirway onditionctatincway isway owkwkwway\n",
            "Epoch:  79 | Train loss: 0.197 | Val loss: 0.886 | Gen: ethay arirway onditioncationcway isway owkwkway\n",
            "Epoch:  80 | Train loss: 0.192 | Val loss: 0.894 | Gen: ethay arirway onditionctay isway owkwkway\n",
            "Epoch:  81 | Train loss: 0.186 | Val loss: 0.906 | Gen: ethay arirway onditiictingnay isway owkwkway\n",
            "Epoch:  82 | Train loss: 0.183 | Val loss: 0.903 | Gen: ethay arirway onditiincay isway owkwkway\n",
            "Epoch:  83 | Train loss: 0.177 | Val loss: 0.917 | Gen: ethay arirway onditiictancway isway owkwkway\n",
            "Epoch:  84 | Train loss: 0.174 | Val loss: 0.920 | Gen: ethay arirway onditiincay isway owkwkway\n",
            "Epoch:  85 | Train loss: 0.170 | Val loss: 0.926 | Gen: ethay arirway onditiincay isway owkwkway\n",
            "Epoch:  86 | Train loss: 0.167 | Val loss: 0.930 | Gen: ethay arirway onditiincay isway owkwkway\n",
            "Epoch:  87 | Train loss: 0.164 | Val loss: 0.935 | Gen: ethay arirway onditiincay isway owkwkway\n",
            "Epoch:  88 | Train loss: 0.161 | Val loss: 0.935 | Gen: ethay arirway onditiincay isway owkwkway\n",
            "Epoch:  89 | Train loss: 0.158 | Val loss: 0.939 | Gen: ethay arirway onditiincay isway owkwkway\n",
            "Epoch:  90 | Train loss: 0.155 | Val loss: 0.936 | Gen: ethay arirway onditiingfay isway owkwkway\n",
            "Epoch:  91 | Train loss: 0.152 | Val loss: 0.942 | Gen: ethay arirway onditiincay isway owkwkway\n",
            "Epoch:  92 | Train loss: 0.149 | Val loss: 0.943 | Gen: ethay arirway onditiingfay isway owkwkway\n",
            "Epoch:  93 | Train loss: 0.146 | Val loss: 0.946 | Gen: ethay arirway onditiingfay isway owkwkway\n",
            "Epoch:  94 | Train loss: 0.143 | Val loss: 0.945 | Gen: ethay arirway onditiingfay isway owkwkway\n",
            "Epoch:  95 | Train loss: 0.141 | Val loss: 0.948 | Gen: ethay arirway onditiincay isway owkwkway\n",
            "Epoch:  96 | Train loss: 0.137 | Val loss: 0.950 | Gen: ethay arirway onditiingfay isway owkwkway\n",
            "Epoch:  97 | Train loss: 0.134 | Val loss: 0.955 | Gen: ethay arirway onditiingfay isway owkwkway\n",
            "Epoch:  98 | Train loss: 0.131 | Val loss: 0.960 | Gen: ethay arirway onditiioncway isway owkwkway\n",
            "Epoch:  99 | Train loss: 0.128 | Val loss: 0.964 | Gen: ethay arirway onditiioncway isway owkwkway\n",
            "Obtained lowest validation loss of: 0.8760785157547185\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay arirway onditiioncway isway owkwkway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l28mKuZxvaRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f9934ad-85c5-42f9-e71b-a0e0c4b607ac"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay arirway onditiioncway isway owkwkway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L8EqLYFu48H"
      },
      "source": [
        "In the following cells, we investigate the effects of increasing model size and dataset size on the training / validation curves and generalization of the Transformer. We will increase hidden size to 64, and also increase dataset size. Include the best achieved validation loss in your report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdZO69DozuUu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "299fbcdd-e267-46b4-e155-86e3c27c8535"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "trans32_args_l = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_large', # Increased data set size\n",
        "              'cuda':True, \n",
        "              'nepochs':100,\n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':5e-4,\n",
        "              'early_stopping_patience': 10,\n",
        "              'lr_decay':0.99,\n",
        "              'batch_size': 512,\n",
        "              'hidden_size': 32,\n",
        "              'encoder_type': 'transformer',\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "trans32_args_l.update(args_dict)\n",
        "print_opts(trans32_args_l)\n",
        "\n",
        "trans32_encoder_l, trans32_decoder_l, trans32_losses_l = train(trans32_args_l)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, trans32_encoder_l, trans32_decoder_l, None, trans32_args_l)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 10                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('spears', 'earsspay')\n",
            "('command', 'ommandcay')\n",
            "('peeing', 'eeingpay')\n",
            "('vibrant', 'ibrantvay')\n",
            "('alchemy', 'alchemyway')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.790 | Val loss: 2.273 | Gen: eeay -ay-ay ion-on-onay inay ioe-ay-onay\n",
            "Epoch:   1 | Train loss: 2.095 | Val loss: 2.018 | Gen: eay ay onay ingsinginy onay\n",
            "Epoch:   2 | Train loss: 1.849 | Val loss: 1.880 | Gen: eay iway ongay isiny ongay\n",
            "Epoch:   3 | Train loss: 1.692 | Val loss: 1.756 | Gen: eway iway intingintinginway isway ongay-ingday\n",
            "Epoch:   4 | Train loss: 1.574 | Val loss: 1.703 | Gen: etay iray intinginininininway isway ongay-ingday\n",
            "Epoch:   5 | Train loss: 1.474 | Val loss: 1.864 | Gen: ettatatetecay iray-iray onnginginginininway isway ongray-ingdingy\n",
            "Epoch:   6 | Train loss: 1.397 | Val loss: 1.639 | Gen: etway iway intinginginay isway ongay\n",
            "Epoch:   7 | Train loss: 1.332 | Val loss: 1.736 | Gen: eway-itecay irararay-iray inginginginginginway isay-isay ongay-ingingingay\n",
            "Epoch:   8 | Train loss: 1.297 | Val loss: 1.548 | Gen: eway iway intinginginginway isway ongay\n",
            "Epoch:   9 | Train loss: 1.212 | Val loss: 1.531 | Gen: etway iraray intingingingingingay isway ongay-ingingay\n",
            "Epoch:  10 | Train loss: 1.152 | Val loss: 1.518 | Gen: etway ariay inginginginginginway isway ongay\n",
            "Epoch:  11 | Train loss: 1.115 | Val loss: 1.656 | Gen: etetway irray intiongingingingiay isiay ourgrgay\n",
            "Epoch:  12 | Train loss: 1.098 | Val loss: 1.500 | Gen: eway aiway intioinginay isway ourway\n",
            "Epoch:  13 | Train loss: 1.047 | Val loss: 1.471 | Gen: eteway arriay ontingingingingiway isway ourgrgay\n",
            "Epoch:  14 | Train loss: 0.998 | Val loss: 1.337 | Gen: etway ariway ontingingingingway isway oungay\n",
            "Epoch:  15 | Train loss: 0.967 | Val loss: 1.558 | Gen: etewayway arriay ontiongingingiay isway ourtay-ingay\n",
            "Epoch:  16 | Train loss: 0.956 | Val loss: 1.315 | Gen: eeway amarway ontionginginay isway oway\n",
            "Epoch:  17 | Train loss: 0.926 | Val loss: 1.282 | Gen: etway ariway ontiongingingiay isway ouongay\n",
            "Epoch:  18 | Train loss: 0.881 | Val loss: 1.308 | Gen: etway away ontiongingway iway oway\n",
            "Epoch:  19 | Train loss: 0.885 | Val loss: 1.286 | Gen: etway arway ontiongingingiay isway orway\n",
            "Epoch:  20 | Train loss: 0.840 | Val loss: 1.460 | Gen: etway away ontingingay iway oway\n",
            "Epoch:  21 | Train loss: 0.827 | Val loss: 1.324 | Gen: etway arway ontingingay isway oway\n",
            "Epoch:  22 | Train loss: 0.781 | Val loss: 1.343 | Gen: etway away ontingay isway oway\n",
            "Epoch:  23 | Train loss: 0.755 | Val loss: 1.253 | Gen: etway arway ontiongingay isway owdway\n",
            "Epoch:  24 | Train loss: 0.721 | Val loss: 1.239 | Gen: etway arway ontingingingay isway owdway\n",
            "Epoch:  25 | Train loss: 0.697 | Val loss: 1.161 | Gen: ethay arway ontingingingiay isway owoway\n",
            "Epoch:  26 | Train loss: 0.673 | Val loss: 1.144 | Gen: ethay arway ontingingingay isway owdway\n",
            "Epoch:  27 | Train loss: 0.650 | Val loss: 1.112 | Gen: ethay arway ontingingingiay isway oworway\n",
            "Epoch:  28 | Train loss: 0.632 | Val loss: 1.082 | Gen: ethay arway ontingingingay isway owdingay\n",
            "Epoch:  29 | Train loss: 0.610 | Val loss: 1.075 | Gen: ethay arway ontingingingiay isway oworway\n",
            "Epoch:  30 | Train loss: 0.596 | Val loss: 1.064 | Gen: ethay arway ondintingiongway isway owdingay\n",
            "Epoch:  31 | Train loss: 0.590 | Val loss: 1.045 | Gen: ethay arway ondintitingiay isway owoway\n",
            "Epoch:  32 | Train loss: 0.570 | Val loss: 1.027 | Gen: ethay iwrway ondintitingiay isway owdingay\n",
            "Epoch:  33 | Train loss: 0.552 | Val loss: 0.999 | Gen: ethay arway ondintitiongngway isway owdingay\n",
            "Epoch:  34 | Train loss: 0.530 | Val loss: 0.993 | Gen: ethay arway onditingingiay isway owkingay\n",
            "Epoch:  35 | Train loss: 0.516 | Val loss: 0.979 | Gen: ethay arway onditingingiay isway owkringay\n",
            "Epoch:  36 | Train loss: 0.501 | Val loss: 0.974 | Gen: ethay arway onditingingay isway owkringway\n",
            "Epoch:  37 | Train loss: 0.487 | Val loss: 0.957 | Gen: ethay arway onditingingiay isway orengingway\n",
            "Epoch:  38 | Train loss: 0.475 | Val loss: 0.954 | Gen: ethay arway onditingingiay isway owdingay\n",
            "Epoch:  39 | Train loss: 0.464 | Val loss: 0.970 | Gen: ethay arway onditingingay isway owdingay\n",
            "Epoch:  40 | Train loss: 0.450 | Val loss: 0.933 | Gen: ethay arway onditiongngay isway owdingay\n",
            "Epoch:  41 | Train loss: 0.443 | Val loss: 0.953 | Gen: ethay arway onditingingiay isway orenginay\n",
            "Epoch:  42 | Train loss: 0.443 | Val loss: 0.909 | Gen: ethay irway onditiongingiay isway owdingray\n",
            "Epoch:  43 | Train loss: 0.459 | Val loss: 1.559 | Gen: ethatway ariayaydayay onditingingiay-onay isswaydayyyyyydayday oriongday-inaydway\n",
            "Epoch:  44 | Train loss: 0.528 | Val loss: 1.124 | Gen: ethay arway onditiongingay isway oredway\n",
            "Epoch:  45 | Train loss: 0.503 | Val loss: 0.892 | Gen: ethay iraway onditingngingway isway owdingay\n",
            "Epoch:  46 | Train loss: 0.431 | Val loss: 0.908 | Gen: ethay arway ondititingay isway owdingay\n",
            "Epoch:  47 | Train loss: 0.421 | Val loss: 0.901 | Gen: ethay irway ondititingngiay isway owdingway\n",
            "Epoch:  48 | Train loss: 0.411 | Val loss: 0.833 | Gen: ethay arway onditiongnginay isway owdinginay\n",
            "Epoch:  49 | Train loss: 0.378 | Val loss: 0.809 | Gen: ethay arway ondititingiongway isway owdingigway\n",
            "Epoch:  50 | Train loss: 0.357 | Val loss: 0.799 | Gen: ethay arway onditiongnginay isway oriongway\n",
            "Epoch:  51 | Train loss: 0.345 | Val loss: 0.800 | Gen: ethay arway ondititingiongway isway owdingway\n",
            "Epoch:  52 | Train loss: 0.340 | Val loss: 0.801 | Gen: ethay arway onditiongnginay isway oriongway\n",
            "Epoch:  53 | Train loss: 0.331 | Val loss: 0.800 | Gen: ethay arway onditiongnginay isway owdingway\n",
            "Epoch:  54 | Train loss: 0.323 | Val loss: 0.794 | Gen: ethay arway onditiongnginay isway oriongway\n",
            "Epoch:  55 | Train loss: 0.315 | Val loss: 0.798 | Gen: ethay arway onditiongingcay isway owdingfay\n",
            "Epoch:  56 | Train loss: 0.308 | Val loss: 0.788 | Gen: ethay arway onditiongnginay isway oriongway\n",
            "Epoch:  57 | Train loss: 0.300 | Val loss: 0.802 | Gen: ethay arway onditiongingcay isway oriongway\n",
            "Epoch:  58 | Train loss: 0.294 | Val loss: 0.781 | Gen: ethay arway onditiongingnay isway oriongway\n",
            "Epoch:  59 | Train loss: 0.289 | Val loss: 0.801 | Gen: ethay arway onditiongingcay isway oriongway\n",
            "Epoch:  60 | Train loss: 0.284 | Val loss: 0.776 | Gen: ethay arway onditiongingnay isway orwiongway\n",
            "Epoch:  61 | Train loss: 0.275 | Val loss: 0.814 | Gen: ethay arway onditiongingcay isway oriongway\n",
            "Epoch:  62 | Train loss: 0.272 | Val loss: 0.772 | Gen: ethay arway onditiongingcay isway oriongway\n",
            "Epoch:  63 | Train loss: 0.263 | Val loss: 0.805 | Gen: ethay arway onditiongingcay isway oriongway\n",
            "Epoch:  64 | Train loss: 0.260 | Val loss: 0.775 | Gen: ethay arway onditiongingcay isway oriongway\n",
            "Epoch:  65 | Train loss: 0.251 | Val loss: 0.801 | Gen: ethay arway onditiongingcay isway oriongway\n",
            "Epoch:  66 | Train loss: 0.247 | Val loss: 0.785 | Gen: ethay ariway onditioniongcay isway oriongway\n",
            "Epoch:  67 | Train loss: 0.239 | Val loss: 0.789 | Gen: ethay arway onditiongingcay isway oriongway\n",
            "Epoch:  68 | Train loss: 0.233 | Val loss: 0.843 | Gen: ethay ariway onditioniongcay isway oriongway\n",
            "Epoch:  69 | Train loss: 0.232 | Val loss: 0.816 | Gen: ethay airway onditioningcay isway oriongway\n",
            "Epoch:  70 | Train loss: 0.225 | Val loss: 0.782 | Gen: ethay arway onditioniongcay isway oriongway\n",
            "Epoch:  71 | Train loss: 0.217 | Val loss: 0.792 | Gen: ethay airway onditioniongcay isway oriongway\n",
            "Epoch:  72 | Train loss: 0.216 | Val loss: 0.802 | Gen: ethay airway onditioniongcay isway orkingway\n",
            "Validation loss has not improved in 10 epochs, stopping early\n",
            "Obtained lowest validation loss of: 0.7720891081404929\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioniongcay isway oriongway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmoTgrDcr_dw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "144c3cd4-99a9-4c66-b11e-fd9bbf4443c4"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "trans64_args_s = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_small',\n",
        "              'cuda':True, \n",
        "              'nepochs':50, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':5e-4,\n",
        "              'early_stopping_patience': 20,\n",
        "              'lr_decay':0.99,\n",
        "              'batch_size': 64, \n",
        "              'hidden_size': 64, # Increased model size\n",
        "              'encoder_type': 'transformer',\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "trans64_args_s.update(args_dict)\n",
        "print_opts(trans64_args_s)\n",
        "\n",
        "trans64_encoder_s, trans64_decoder_s, trans64_losses_s = train(trans64_args_s)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, trans64_encoder_s, trans64_decoder_s, None, trans64_args_s)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 20                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('brother', 'otherbray')\n",
            "('command', 'ommandcay')\n",
            "('collecting', 'ollectingcay')\n",
            "('follows', 'ollowsfay')\n",
            "('seems', 'eemssay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.447 | Val loss: 1.946 | Gen: atetetetay iay ingway iway ingway\n",
            "Epoch:   1 | Train loss: 1.599 | Val loss: 1.575 | Gen: etay iray ongatingay isisssisisisisisisay ongay\n",
            "Epoch:   2 | Train loss: 1.313 | Val loss: 1.399 | Gen: eay irway ongngatingngnay isway onglingray\n",
            "Epoch:   3 | Train loss: 1.116 | Val loss: 1.331 | Gen: eteway irway ontingngngngnay isway orkgngngay\n",
            "Epoch:   4 | Train loss: 0.980 | Val loss: 1.266 | Gen: ehay irway ondlingngingntindayn isway olingngngway\n",
            "Epoch:   5 | Train loss: 0.867 | Val loss: 1.199 | Gen: ehay arway ondintingcay isway orkingngway\n",
            "Epoch:   6 | Train loss: 0.766 | Val loss: 1.018 | Gen: ehay arway onditingnay isway orkingway\n",
            "Epoch:   7 | Train loss: 0.634 | Val loss: 0.922 | Gen: ehehay arway onditingway isay orkingway\n",
            "Epoch:   8 | Train loss: 0.564 | Val loss: 1.090 | Gen: ehay arway onditiongiongcay isway orkingway\n",
            "Epoch:   9 | Train loss: 0.558 | Val loss: 0.905 | Gen: ethay arway onditingway isway owkwagngway\n",
            "Epoch:  10 | Train loss: 0.477 | Val loss: 0.801 | Gen: ethay arway onditingway isway owkingngway\n",
            "Epoch:  11 | Train loss: 0.367 | Val loss: 0.797 | Gen: ethay arway onditingongcay isway orkingngway\n",
            "Epoch:  12 | Train loss: 0.320 | Val loss: 0.740 | Gen: ehay arway onditingoningcay isway owkingway\n",
            "Epoch:  13 | Train loss: 0.270 | Val loss: 0.807 | Gen: ethay arway onditingnionnnnay isway orkingnay\n",
            "Epoch:  14 | Train loss: 0.260 | Val loss: 0.731 | Gen: ehay arway onditingingcay isway owingtway\n",
            "Epoch:  15 | Train loss: 0.225 | Val loss: 0.841 | Gen: ethay away ondcitingcaintiocay iway owkingway\n",
            "Epoch:  16 | Train loss: 0.295 | Val loss: 0.821 | Gen: ehay away onditiongcay iway owkingway\n",
            "Epoch:  17 | Train loss: 0.225 | Val loss: 0.746 | Gen: ehthay arway onditiongcay isway orkingway\n",
            "Epoch:  18 | Train loss: 0.192 | Val loss: 0.908 | Gen: ethay arway ondintiongcay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.176 | Val loss: 0.652 | Gen: ethay arirway onditiongcay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.148 | Val loss: 0.689 | Gen: ethay arway onditiongcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.133 | Val loss: 0.773 | Gen: ehay arwwway ondcitiongcway way orkway\n",
            "Epoch:  22 | Train loss: 0.272 | Val loss: 1.040 | Gen: ethtay iraway ondinitiongcay isay oorkingway\n",
            "Epoch:  23 | Train loss: 0.278 | Val loss: 0.741 | Gen: ehay arrway ondcitiongcay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.201 | Val loss: 0.653 | Gen: ethay ariway onditiongcingcay isway okway\n",
            "Epoch:  25 | Train loss: 0.132 | Val loss: 0.612 | Gen: ehtay iraway onditiongcincay isway okway\n",
            "Epoch:  26 | Train loss: 0.097 | Val loss: 0.555 | Gen: ehthay iriway onditiongcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.078 | Val loss: 0.548 | Gen: ehtay arirway onditiongcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.065 | Val loss: 0.559 | Gen: ehtay arirway onditiongcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.054 | Val loss: 0.573 | Gen: ehtay arirway onditiongcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.047 | Val loss: 0.577 | Gen: ehthay arirway onditiongcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.042 | Val loss: 0.589 | Gen: ehthay arirway onditiongcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.039 | Val loss: 0.563 | Gen: ehtay arrway onditiongcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.033 | Val loss: 0.589 | Gen: ehtay arrway onditiongcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.030 | Val loss: 0.604 | Gen: ehthay arirway onditingcincay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.028 | Val loss: 0.621 | Gen: ehthay arrway onditingcincay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.026 | Val loss: 0.631 | Gen: ehthay arrway onditiongcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.027 | Val loss: 0.659 | Gen: ehthay iraway onditiongcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.042 | Val loss: 0.707 | Gen: ethay ariway onditiniongcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.100 | Val loss: 1.140 | Gen: tethay irway onditionioncay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.134 | Val loss: 0.836 | Gen: eehay ariway onditiongingcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.125 | Val loss: 0.716 | Gen: ehay arriway onditiongcincay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.093 | Val loss: 0.833 | Gen: ehthay aiway onditiongcay isway okingkay\n",
            "Epoch:  43 | Train loss: 0.105 | Val loss: 0.645 | Gen: ethay arrway onditingcingcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.069 | Val loss: 0.580 | Gen: eethay arrway onditingcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.047 | Val loss: 0.565 | Gen: ehthay ariway onditiongcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.039 | Val loss: 0.538 | Gen: ethay arirway onditiongcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.028 | Val loss: 0.502 | Gen: ethay arirway onditiongcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.017 | Val loss: 0.512 | Gen: ethay arrway onditiongcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.014 | Val loss: 0.495 | Gen: ethay arirway onditiongcay isway orkingway\n",
            "Obtained lowest validation loss of: 0.4946705880329797\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay arirway onditiongcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dardK4RWvUWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9df5e4a-cb27-4b54-f5a8-c63ac9a3d5a9"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "trans64_args_l = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_large', # Increased data set size\n",
        "              'cuda':True, \n",
        "              'nepochs':50,\n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':5e-4,\n",
        "              'early_stopping_patience': 20,\n",
        "              'lr_decay':0.99,\n",
        "              'batch_size': 512, \n",
        "              'hidden_size': 64, # Increased model size\n",
        "              'encoder_type': 'transformer',\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "trans64_args_l.update(args_dict)\n",
        "print_opts(trans64_args_l)\n",
        "\n",
        "trans64_encoder_l, trans64_decoder_l, trans64_losses_l = train(trans64_args_l)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, trans64_encoder_l, trans64_decoder_l, None, trans64_args_l)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 20                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('spears', 'earsspay')\n",
            "('command', 'ommandcay')\n",
            "('peeing', 'eeingpay')\n",
            "('vibrant', 'ibrantvay')\n",
            "('alchemy', 'alchemyway')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.377 | Val loss: 2.098 | Gen: etttttertttttttttttt ay-ay-ay ongongondngngngdngda ssssmmmmmmmmmmmmmmay oboboboboblobobobobo\n",
            "Epoch:   1 | Train loss: 1.671 | Val loss: 1.738 | Gen: hthththththththtttth aray ongongongngngontiona isssssssssssssssssay ogogogogogggogogy\n",
            "Epoch:   2 | Train loss: 1.422 | Val loss: 1.603 | Gen: etay-eay aray ongay-iongay isssay ogggggggggggay\n",
            "Epoch:   3 | Train loss: 1.296 | Val loss: 1.749 | Gen: ertay-ay-ay arwayraywaywaywayway ongay-ionay isssay ogoggay-ay-ay\n",
            "Epoch:   4 | Train loss: 1.232 | Val loss: 1.519 | Gen: ehthtay araway ontndndntionontionco issay orgay\n",
            "Epoch:   5 | Train loss: 1.080 | Val loss: 1.602 | Gen: ehthay-e ardi-aray ondgay-ongay isway ogkay-ggggway\n",
            "Epoch:   6 | Train loss: 1.002 | Val loss: 1.417 | Gen: ehtway araway ondondodondoway isway okowray\n",
            "Epoch:   7 | Train loss: 0.905 | Val loss: 1.431 | Gen: ehththteway aray ongay-inay isay okay-igray\n",
            "Epoch:   8 | Train loss: 0.848 | Val loss: 1.306 | Gen: ethtway arwaywayayay ontaygay iswaywayway oooogay\n",
            "Epoch:   9 | Train loss: 0.781 | Val loss: 1.113 | Gen: ethay ariway onditigitingigway isway ogay-igway\n",
            "Epoch:  10 | Train loss: 0.690 | Val loss: 1.178 | Gen: ehtway arway ondongday isway ogay\n",
            "Epoch:  11 | Train loss: 0.654 | Val loss: 0.998 | Gen: ehtay ariway ondingitigingway isway ogay-ingway\n",
            "Epoch:  12 | Train loss: 0.597 | Val loss: 1.000 | Gen: ehtway ariway ondonitigingcay isway ooogay\n",
            "Epoch:  13 | Train loss: 0.533 | Val loss: 1.007 | Gen: ehtway ariway onditininigcay isway okway\n",
            "Epoch:  14 | Train loss: 0.493 | Val loss: 0.864 | Gen: ehtway iway onditionigcay isway oowingway\n",
            "Epoch:  15 | Train loss: 0.429 | Val loss: 0.815 | Gen: ehtway ariway ondocinitingway isway owrkingway\n",
            "Epoch:  16 | Train loss: 0.397 | Val loss: 0.901 | Gen: ehtway airway onditingcay isway okragingway\n",
            "Epoch:  17 | Train loss: 0.391 | Val loss: 1.017 | Gen: ethay airway ontinditingcay isway oooorkngway\n",
            "Epoch:  18 | Train loss: 0.398 | Val loss: 0.891 | Gen: ehtay airway onditiningcay isway okingway\n",
            "Epoch:  19 | Train loss: 0.331 | Val loss: 0.813 | Gen: ehtway irway ondocinigcay isway oorkingway\n",
            "Epoch:  20 | Train loss: 0.303 | Val loss: 0.865 | Gen: ehtay irway onditioniningcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.279 | Val loss: 0.690 | Gen: ehtway airway onditioningcay isway oorkingway\n",
            "Epoch:  22 | Train loss: 0.237 | Val loss: 0.790 | Gen: ehtway airway onditioningcay isway okingway\n",
            "Epoch:  23 | Train loss: 0.224 | Val loss: 0.888 | Gen: ehtway airway onditioningcay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.251 | Val loss: 1.019 | Gen: ethay irway onditionitingcay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.329 | Val loss: 0.930 | Gen: ehay aiwaywaaay onditioningcay isway oorkingway\n",
            "Epoch:  26 | Train loss: 0.287 | Val loss: 0.804 | Gen: ehtway irway onditiongcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.259 | Val loss: 0.942 | Gen: ehtay arway ondctioningcay isway oorkingway\n",
            "Epoch:  28 | Train loss: 0.269 | Val loss: 0.723 | Gen: ehtay irway onditioningcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.208 | Val loss: 0.633 | Gen: ehtway airway onditioningcay isway oorkingway\n",
            "Epoch:  30 | Train loss: 0.167 | Val loss: 0.534 | Gen: ehtway airway onditioningcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.138 | Val loss: 0.535 | Gen: ehtay arirway onditioningcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.116 | Val loss: 0.527 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.108 | Val loss: 0.537 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.100 | Val loss: 0.527 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.090 | Val loss: 0.520 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.079 | Val loss: 0.530 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.071 | Val loss: 0.532 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.066 | Val loss: 0.524 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.066 | Val loss: 0.562 | Gen: ehtay airrway onditioningcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.070 | Val loss: 0.561 | Gen: ethay airrway onditiongcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.062 | Val loss: 0.532 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.101 | Val loss: 1.416 | Gen: yhay ariray ondidintiongcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.334 | Val loss: 1.074 | Gen: theay iwirway onoditiongcay isway oookingway\n",
            "Epoch:  44 | Train loss: 0.265 | Val loss: 0.718 | Gen: ehtway arirway onditionigingcay iswway orkingway\n",
            "Epoch:  45 | Train loss: 0.145 | Val loss: 0.484 | Gen: ehtay airway onditionongcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.078 | Val loss: 0.421 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.057 | Val loss: 0.414 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.048 | Val loss: 0.413 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.042 | Val loss: 0.415 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Obtained lowest validation loss of: 0.41348481103682855\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSSyiG39vVlN"
      },
      "source": [
        "The following cell generates two loss plots. In the first plot, we compare the effects of increasing dataset size. In the second plot, we compare the effects of increasing model size. Include both plots in your report, and include your analysis of the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ql0pxrEvVP6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3dd930c6-ea28-48ed-d771-69af68396419"
      },
      "source": [
        "save_loss_comparison_by_dataset(trans32_losses_s, trans32_losses_l, trans64_losses_s, trans64_losses_l, trans32_args_s, trans32_args_l, trans64_args_s, trans64_args_l, 'trans_by_dataset')\n",
        "save_loss_comparison_by_hidden(trans32_losses_s, trans32_losses_l, trans64_losses_s, trans64_losses_l, trans32_args_s, trans32_args_l, trans64_args_s, trans64_args_l, 'trans_by_hidden')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBnBXRG8mvcn"
      },
      "source": [
        "# Optional: Attention Visualizations\n",
        "\n",
        "One of the benefits of using attention is that it allows us to gain insight into the inner workings of the model.\n",
        "\n",
        "By visualizing the attention weights generated for the input tokens in each decoder step, we can see where the model focuses while producing each output token.\n",
        "\n",
        "The code in this section loads the model you trained from the previous section and uses it to translate a given set of words: it prints the translations and display heatmaps to show how attention is used at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqEC0vN9mvpV"
      },
      "source": [
        "## Step 1: Visualize Attention Masks\n",
        "Play around with visualizing attention maps generated by the previous two models you've trained. Inspect visualizations in one success and one failure case for both models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkfz-u-MtudL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "43909a3f-883a-49aa-e9e2-6931ec2ce0ac"
      },
      "source": [
        "TEST_WORD_ATTN = 'street'\n",
        "visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUEAAAEYCAYAAADCj0QOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXATNrNJcBM2s0v4/SKcahrJChysAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eedday'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ssa7g35zt2yj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        },
        "outputId": "34b5b248-3490-4853-99aa-0146ce25681f"
      },
      "source": [
        "TEST_WORD_ATTN = 'street'\n",
        "visualize_attention(TEST_WORD_ATTN, transformer_encoder, transformer_decoder, None, args, )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEhCAYAAABRKfYcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB8PovBek4HPaAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEhCAYAAABRKfYcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEjaLggwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEhCAYAAABRKfYcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eetstray'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    }
  ]
}