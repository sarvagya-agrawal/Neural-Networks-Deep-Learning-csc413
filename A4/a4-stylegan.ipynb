{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "a4-stylegan.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YkGmGCi6oRu"
      },
      "source": [
        "## **University of Toronto - CSC413 - Neural Networks and Deep Learning** \n",
        "## **Programming Assignment 4 - StyleGAN2-Ada**\n",
        "\n",
        "This is a self-contained notebook that allows you to play around with a pre-trained StyleGAN2-Ada generator\n",
        "\n",
        "Disclaimer: Some codes were borrowed from StyleGAN official documentation on Github https://github.com/NVlabs/stylegan\n",
        "\n",
        "Make sure to set your runtime to GPU\n",
        "\n",
        "Remember to save your progress periodically!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-W5eReUVPnS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac3e20bf-c509-498b-e656-4218e011e382"
      },
      "source": [
        "# Run this for Google CoLab (use TensorFlow 1.x)\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "# clone StyleGAN2 Ada\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Cloning into 'stylegan2-ada'...\n",
            "remote: Enumerating objects: 71, done.\u001b[K\n",
            "remote: Total 71 (delta 0), reused 0 (delta 0), pack-reused 71\u001b[K\n",
            "Unpacking objects: 100% (71/71), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVNM_ERtVxA1"
      },
      "source": [
        "#setup some environments (Do not change any of the following)\n",
        "import sys\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "from IPython.display import Image\n",
        "import PIL.Image\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sys.path.insert(0, \"/content/stylegan2-ada\") #do not remove this line\n",
        "\n",
        "import dnnlib\n",
        "import dnnlib.tflib as tflib\n",
        "import IPython.display\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhU-cjsC6CdN"
      },
      "source": [
        "Next, we will load a pre-trained StyleGan2-ada network.\n",
        "\n",
        "Each of the following pre-trained network is specialized to generate one type of image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENrDl7ZTddyT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3af91a3-b4dd-43b5-8ef3-ee333510c495"
      },
      "source": [
        "# The pre-trained networks are stored as standard pickle files\n",
        "# Uncomment one of the following URL to begin\n",
        "# If you wish, you can also find other pre-trained networks online\n",
        "\n",
        "URL = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl\"      # Human faces\n",
        "#URL = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/cifar10.pkl\"  # CIFAR10, these images are a bit too tiny for our experiment \n",
        "#URL = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/afhqwild.pkl\" # wild animal pictures\n",
        "#URL = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/metfaces.pkl\" # European portrait paintings\n",
        "#URL = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/afhqcat.pkl\"  # cats\n",
        "#URL = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/afhqdog.pkl\"  # dogs\n",
        "tflib.init_tf() #this creates a default Tensorflow session\n",
        "\n",
        "# we are now going to load the StyleGAN2-Ada model \n",
        "# The following code downloads the file and unpickles it to yield 3 instances of dnnlib.tflib.Network. \n",
        "with dnnlib.util.open_url(URL) as fp:\n",
        "    _G, _D, Gs = pickle.load(fp) \n",
        "# Here is a brief description of _G, _D, Gs, for details see the official StyleGAN documentation \n",
        "# _G = Instantaneous snapshot of the generator. Mainly useful for resuming a previous training run.\n",
        "# _D = Instantaneous snapshot of the discriminator. Mainly useful for resuming a previous training run.\n",
        "# Gs = Long-term average of the generator. Yields higher-quality results than the instantaneous snapshot.\n",
        "# We will work with Gs "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl ... done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk3EKnIyckPE"
      },
      "source": [
        "## Part 1 Sampling and Identifying Fakes \n",
        "\n",
        "Open: https://github.com/NVlabs/stylegan and follow the instructions starting from *There are three ways to use the pre-trained generator....*\n",
        "\n",
        "Complete generate_latent_code and generate_images function in the Colab notebook to generate a small row of $3 - 5$ images. \n",
        "\n",
        "You do not need to include these images into your PDF submission. \n",
        "\n",
        "If you wish, you can try to use https://www.whichfaceisreal.com/learn.html as a guideline to spot any imperfections that you detect in these images, e.g., ``blob artifact\" and make a short remark for your attached images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7eAfVJDiB0y"
      },
      "source": [
        "# Sample a batch of latent codes {z_1, ...., z_B}, B is your batch size.  \n",
        "def generate_latent_code(SEED, BATCH, LATENT_DIMENSION = 512):\n",
        "  \"\"\"\n",
        "  This function returns a sample a batch of 512 dimensional random latent code\n",
        "\n",
        "  - SEED: int\n",
        "  - BATCH: int that specifies the number of latent codes, Recommended batch_size is 3 - 6\n",
        "  - LATENT_DIMENSION is by default 512 (see Karras et al.)\n",
        "  \n",
        "  You should use np.random.RandomState to construct a random number generator, say rnd\n",
        "  Then use rnd.randn along with your BATCH and LATENT_DIMENSION to generate your latent codes. \n",
        "  This samples a batch of latent codes from a normal distribution \n",
        "  https://numpy.org/doc/stable/reference/random/generated/numpy.random.RandomState.randn.html\n",
        "  \n",
        "  Return latent_codes, which is a 2D array with dimensions BATCH times LATENT_DIMENSION \n",
        "  \"\"\"\n",
        "  ################################################################################\n",
        "  ########################## COMPLETE THE FOLLOWING ##############################\n",
        "  ################################################################################\n",
        "  rnd = np.random.RandomState(SEED)\n",
        "  latent_codes = rnd.randn(BATCH, LATENT_DIMENSION)\n",
        "  ################################################################################\n",
        "  return latent_codes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8giN3BfibqG"
      },
      "source": [
        "# Sample images from your latent codes https://github.com/NVlabs/stylegan\n",
        "# You can use their default settings\n",
        "\n",
        "################################################################################\n",
        "########################## COMPLETE THE FOLLOWING ##############################\n",
        "################################################################################\n",
        "def generate_images(SEED, BATCH, TRUNCATION = 0.7):\n",
        "  \"\"\"\n",
        "  This function generates a batch of images from latent codes. \n",
        "  \n",
        "  - SEED: int\n",
        "  - BATCH: int that specifies the number of latent codes to be generated\n",
        "  - TRUNCATION: float between [-1, 1] that decides the amount of clipping to apply to the latent code distribution\n",
        "              recommended setting is 0.7\n",
        "\n",
        "  You will use Gs.run() to sample images. See https://github.com/NVlabs/stylegan for details\n",
        "  You may use their default setting.  \n",
        "  \"\"\" \n",
        "  # Sample a batch of latent code z using generate_latent_code function\n",
        "  latent_codes = generate_latent_code(SEED = SEED, BATCH= BATCH)\n",
        "\n",
        "  # Convert latent code into images by following https://github.com/NVlabs/stylegan\n",
        "  fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True) \n",
        "  images = Gs.run(latent_codes, None, truncation_psi=0.7, randomize_noise=True, output_transform=fmt)\n",
        "\n",
        "  return PIL.Image.fromarray(np.concatenate(images, axis=1) , 'RGB')\n",
        "################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdaxzuAFcTM8"
      },
      "source": [
        "# Generate your images\n",
        "generate_images(2, 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9Jre5OXbUHD"
      },
      "source": [
        "## **Part 2 Interpolation**\n",
        "\n",
        "Complete the interpolate_images function using linear interpolation between two latent codes,\n",
        "\\begin{equation}\n",
        "    z = r z_1 + (1-r) z_2,  r \\in [0, 1]\n",
        "\\end{equation}\n",
        "and feeding this interpolation through the StyleGAN2-Ada generator Gs as done in generate_images. Include a small row of interpolation in your PDF submission as a screen shot if necessary to keep the file size small. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUwCVd9mlDOr"
      },
      "source": [
        "################################################################################\n",
        "########################## COMPLETE THE FOLLOWING ##############################\n",
        "################################################################################\n",
        "def interpolate_images(SEED1, SEED2, INTERPOLATION, BATCH = 1, TRUNCATION = 0.7):\n",
        "  \"\"\"\n",
        "  - SEED1, SEED2: int, seed to use to generate the two latent codes\n",
        "  - INTERPOLATION: int, the number of interpolation between the two images, recommended setting 6 - 10\n",
        "  - BATCH: int, the number of latent code to generate. In this experiment, it is 1. \n",
        "  - TRUNCATION: float between [-1, 1] that decides the amount of clipping to apply to the latent code distribution\n",
        "              recommended setting is 0.7\n",
        "\n",
        "  You will interpolate between two latent code that you generate using the above formula\n",
        "  You can generate an interpolation variable using np.linspace\n",
        "  https://numpy.org/doc/stable/reference/generated/numpy.linspace.html\n",
        "  \n",
        "  This function should return an interpolated image. Include a screenshot in your submission.\n",
        "  \"\"\"\n",
        "  latent_code_1 = np.repeat(generate_latent_code(SEED = SEED1, BATCH = BATCH), INTERPOLATION, axis = 0)\n",
        "  latent_code_2 = np.repeat(generate_latent_code(SEED = SEED2, BATCH = BATCH), INTERPOLATION, axis = 0)\n",
        "  interpolate_var = np.linspace(0,1, num=INTERPOLATION).reshape(INTERPOLATION, 1)\n",
        "  interpolated_latent = (interpolate_var * latent_code_1) + (np.flip(interpolate_var) * latent_code_2)\n",
        "  fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "  images = Gs.run(interpolated_latent, None, truncation_psi=0.7, randomize_noise=True, output_transform=fmt)\n",
        "  \n",
        "  return PIL.Image.fromarray(np.concatenate(images, axis=1) , 'RGB')\n",
        "################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV4Df5J8lZLy"
      },
      "source": [
        "# Create an interpolation of your generated images\n",
        "interpolate_images(1,4, INTERPOLATION= 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5HUpzB0T5LN"
      },
      "source": [
        "After you have generated interpolated images, an interesting task would be to see how you can create a GIF. Feel free to explore a little bit more. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV2tsaAhaTzg"
      },
      "source": [
        "## **Part 3 Style Mixing and Fine Control**\n",
        "In the final part, you will reproduce the famous style mixing example from the original StyleGAN paper.\n",
        "\n",
        "### Step 1. We will first learn how to generate from sub-networks of the StyleGAN generator. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG-xbj9qtMTA"
      },
      "source": [
        "# You will generate images from sub-networks of the StyleGAN generator\n",
        "# Similar to Gs, the sub-networks are represented as independent instances of dnnlib.tflib.Network\n",
        "# Complete the function by following \\url{https://github.com/NVlabs/stylegan} \n",
        "# And Look up Gs.components.mapping,  Gs.components.synthesism, Gs.get_var\n",
        "# Remember to use the truncation trick as described in the handout after you obtain src_dlatents from Gs.components.mapping.run\n",
        "def generate_from_subnetwork(src_seeds, LATENT_DIMENSION = 512):\n",
        "    \"\"\"\n",
        "    - src_seeds: a list of int, where each int is used to generate a latent code, e.g., [1,2,3]\n",
        "    - LATENT_DIMENSION: by default 512\n",
        "\n",
        "    You will complete the code snippet in the Write Your Code Here block\n",
        "    This generates several images from a sub-network of the genrator. \n",
        "\n",
        "    To prevent mistakes, we have provided the variable names which corresponds to the ones in the StyleGAN documentation\n",
        "    You should use their convention. \n",
        "    \"\"\"\n",
        "\n",
        "    # default arguments to Gs.components.synthesis.run, this is given to you. \n",
        "    synthesis_kwargs = {\n",
        "        'output_transform': dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True),\n",
        "        'randomize_noise': False,\n",
        "        'minibatch_size': 4\n",
        "    }\n",
        "    ############################################################################\n",
        "    ########################## WRITE YOUR CODE HERE ############################\n",
        "    ############################################################################\n",
        "    truncation = 0.7\n",
        "    src_latents = np.stack(np.random.RandomState(seed).randn(Gs.input_shape[1]) for seed in src_seeds)\n",
        "    src_dlatents = Gs.components.mapping.run(src_latents, None) \n",
        "    w_avg = Gs.get_var('dlatent_avg')\n",
        "    src_dlatents = w_avg + (src_dlatents - w_avg)*truncation\n",
        "    all_images = Gs.components.synthesis.run(src_dlatents, **synthesis_kwargs)\n",
        "    ############################################################################\n",
        "    return PIL.Image.fromarray(np.concatenate(all_images, axis=1) , 'RGB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BI0vEoYdtbJk"
      },
      "source": [
        "# generate several iamges from the sub-network\n",
        "generate_from_subnetwork([1,2,3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QOHpSmoGrZI"
      },
      "source": [
        "### Step 2. Initialize the col_seeds, row_seeds and col_styles and generate a grid of image. \n",
        "\n",
        "A recommended example for your experiment is as follows:\n",
        "\n",
        "*  col_seeds = [1, 2, 3, 4, 5]\n",
        "*  row_seeds = [6]\n",
        "*  col_styles = [1, 2, 3, 4, 5]\n",
        "\n",
        "and\n",
        "\n",
        "*  col_seeds = [1, 2, 3, 4, 5]\n",
        "*  row_seeds = [6]\n",
        "*  col_styles = [8, 9, 10, 11, 12]\n",
        "\n",
        "You will then incorporate your code from generate from sub_network into the cell below. \n",
        "\n",
        "Experiment with the col_styles variable. Explain what col_styles does, for instance, roughly describe what these numbers corresponds to. Create a simple experiment to backup your argument. Include **at maximum two** sets of images that illustrates the effect of changing col_styles and your explanation. Include them as screen shots to minimize the size of the file.\n",
        "\n",
        "Make reference to the original StyleGAN or the StyleGAN2 paper by Karras et al. as needed https://arxiv.org/pdf/1812.04948.pdf https://arxiv.org/pdf/1912.04958.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM4QvAkuwCjW"
      },
      "source": [
        "################################################################################\n",
        "####################COMPLETE THE NEXT THREE LINES###############################\n",
        "################################################################################\n",
        "col_seeds = [1, 2, 3, 4, 5]\n",
        "row_seeds = [6]\n",
        "col_styles = [13, 14, 15, 16, 17]\n",
        "################################################################################\n",
        "src_seeds = list(set(row_seeds + col_seeds))\n",
        "\n",
        "# default arguments to Gs.components.synthesis.run, do not change\n",
        "synthesis_kwargs = {\n",
        "    'output_transform': dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True),\n",
        "    'randomize_noise': False,\n",
        "    'minibatch_size': 4\n",
        "}\n",
        "################################################################################\n",
        "########################## COMPLETE THE FOLLOWING ##############################\n",
        "################################################################################\n",
        "# Copy the   #### WRITE YOUR CODE HERE #### portion from generate_from_subnetwork()\n",
        "truncation = 0.7\n",
        "src_latents = np.stack(np.random.RandomState(seed).randn(Gs.input_shape[1]) for seed in src_seeds)\n",
        "src_dlatents = Gs.components.mapping.run(src_latents, None) \n",
        "w_avg = Gs.get_var('dlatent_avg')\n",
        "src_dlatents = w_avg + (src_dlatents - w_avg)*truncation\n",
        "all_images = Gs.components.synthesis.run(src_dlatents, **synthesis_kwargs)\n",
        "################################################################################\n",
        "\n",
        "# (Do not change)\n",
        "image_dict = {(seed, seed): image for seed, image in zip(src_seeds, list(all_images))}\n",
        "w_dict = {seed: w for seed, w in zip(src_seeds, list(src_dlatents))} \n",
        "\n",
        "# Generating Images (Do not Change)\n",
        "for row_seed in row_seeds:\n",
        "    for col_seed in col_seeds:\n",
        "        w = w_dict[row_seed].copy()\n",
        "        w[col_styles] = w_dict[col_seed][col_styles]\n",
        "        image = Gs.components.synthesis.run(w[np.newaxis], **synthesis_kwargs)[0]\n",
        "        image_dict[(row_seed, col_seed)] = image\n",
        "\n",
        "# Create an Image Grid (Do not Change)\n",
        "def create_grid_images(): \n",
        "  _N, _C, H, W = Gs.output_shape\n",
        "  canvas = PIL.Image.new('RGB', (W * (len(col_seeds) + 1), H * (len(row_seeds) + 1)), 'black')\n",
        "  for row_idx, row_seed in enumerate([None] + row_seeds):\n",
        "      for col_idx, col_seed in enumerate([None] + col_seeds):\n",
        "          if row_seed is None and col_seed is None:\n",
        "              continue\n",
        "          key = (row_seed, col_seed)\n",
        "          if row_seed is None:\n",
        "              key = (col_seed, col_seed)\n",
        "          if col_seed is None:\n",
        "              key = (row_seed, row_seed)\n",
        "          canvas.paste(PIL.Image.fromarray(image_dict[key], 'RGB'), (W * col_idx, H * row_idx))\n",
        "  return canvas\n",
        "\n",
        "# The following code will create your image, save it as a png, and display the image\n",
        "# Run the following code after you have set your row_seed, col_seed and col_style\n",
        "image_grid = create_grid_images()\n",
        "image_grid.save('image_grid.png')\n",
        "im = Image.open(\"image_grid.png\")\n",
        "im"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}